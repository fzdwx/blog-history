[{"content":"主页:\n\u0026nbsp;课程主页 \u0026nbsp;cs diy 主页 视频资源 \u0026nbsp;第4和5集 \u0026nbsp;第6和7集 \u0026nbsp;缺了看这个 \u0026nbsp;有一些 \u0026nbsp;2021 版本 \u0026nbsp;中文翻译 个人记录 我做 lab 时积累的一些相关的 blog\nMap Reduce \u0026nbsp;https://fzdwx.github.io/posts/2022-09-27-mit6.824-lab1/ Raft KV \u0026nbsp;https://fzdwx.github.io/posts/2022-10-10-raftkv/ ","permalink":"https://fzdwx.github.io/notes/9/","summary":"主页:\n\u0026nbsp;课程主页 \u0026nbsp;cs diy 主页 视频资源 \u0026nbsp;第4和5集 \u0026nbsp;第6和7集 \u0026nbsp;缺了看这个 \u0026nbsp;有一些 \u0026nbsp;2021 版本 \u0026nbsp;中文翻译 个人记录 我做 lab 时积累的一些相关的 blog\nMap Reduce \u0026nbsp;https://fzdwx.github.io/posts/2022-09-27-mit6.824-lab1/ Raft KV \u0026nbsp;https://fzdwx.github.io/posts/2022-10-10-raftkv/ ","title":"6.824"},{"content":"刚刚改\u0026nbsp;nvim 配置时，不知道怎么回事，出现了游离分支(英文名称大概是 detached head )。主要症状就是 git提交不了，由于我用的 lazygit ，它的提示不明显，我以为提交了。然后后面打开 idea 看到底是什么。发现提示是游离分支，这个时候我也没在意。 最致命的操作来了: 我直接 checkout main 分支，然后今天晚上修改的记录全没了。\n我的解决方案:\n查看git log，只能显示当前分支的修改 commit 记录。 无效 利用 idea 的 local history 功能，只能找到文件，文件里面的内容是空白的。 无效 搜索 detached head 的解决方案，只能搜到没切换分支前的解决方案，不适用与我的现象。 无效 最后搜索 git 查看所有 commit ，找到了:git reflog，它能显示可引用的历史版本记录，最后找到我要的那个 commit 之后直接git rest --hard xxx完美解决。 说实话，有点慌也有点烦，如果找不回来我可能会弃坑 nvim 了。\n","permalink":"https://fzdwx.github.io/posts/2022-10-23-about-git-detached-head/","summary":"如何在切换了分支之后恢复游离分支提交的代码。","title":"关于 git 游离分支"},{"content":"教程 \u0026nbsp;mdn css 书籍 \u0026nbsp;css in depth ","permalink":"https://fzdwx.github.io/notes/8/","summary":"教程 \u0026nbsp;mdn css 书籍 \u0026nbsp;css in depth ","title":"前端资料"},{"content":"资料 \u0026nbsp;Rust官方文档翻译项目组 ，内含一系列相关文档 \u0026nbsp;Rust语言圣经 \u0026nbsp;Rust resource 项目 \u0026nbsp;pingcap推出的project ，以 Talent Plan 开源数据库开发课程为依托，帮助学习者掌握数据库开发的理论知识，进行实际数据库开发锻炼 ","permalink":"https://fzdwx.github.io/notes/7/","summary":"资料 \u0026nbsp;Rust官方文档翻译项目组 ，内含一系列相关文档 \u0026nbsp;Rust语言圣经 \u0026nbsp;Rust resource 项目 \u0026nbsp;pingcap推出的project ，以 Talent Plan 开源数据库开发课程为依托，帮助学习者掌握数据库开发的理论知识，进行实际数据库开发锻炼 ","title":"Rust 相关资源集合"},{"content":"关于jyy的计算机系统基础课\n主要资料 pa地址: \u0026nbsp;https://nju-projectn.github.io/ics-pa-gitbook/ics2021/ 课程视频地址: \u0026nbsp;https://www.bilibili.com/video/BV1qa4y1j7xk 课程主页: \u0026nbsp;http://jyywiki.cn/ICS/2021/ NEMU ISA相关的API: \u0026nbsp;https://nju-projectn.github.io/ics-pa-gitbook/ics2021/nemu-isa-api.html 扩展资料 Linux C编程一站式学习: \u0026nbsp;https://docs.huihoo.com/c/linux-c-programming/ 计算机教育中缺失的一课: \u0026nbsp;https://missing-semester-cn.github.io/ TODO pa1: \u0026nbsp;https://nju-projectn.github.io/ics-pa-gitbook/ics2021/PA1.html ","permalink":"https://fzdwx.github.io/notes/6/","summary":"关于jyy的计算机系统基础课\n主要资料 pa地址: \u0026nbsp;https://nju-projectn.github.io/ics-pa-gitbook/ics2021/ 课程视频地址: \u0026nbsp;https://www.bilibili.com/video/BV1qa4y1j7xk 课程主页: \u0026nbsp;http://jyywiki.cn/ICS/2021/ NEMU ISA相关的API: \u0026nbsp;https://nju-projectn.github.io/ics-pa-gitbook/ics2021/nemu-isa-api.html 扩展资料 Linux C编程一站式学习: \u0026nbsp;https://docs.huihoo.com/c/linux-c-programming/ 计算机教育中缺失的一课: \u0026nbsp;https://missing-semester-cn.github.io/ TODO pa1: \u0026nbsp;https://nju-projectn.github.io/ics-pa-gitbook/ics2021/PA1.html ","title":"恶补计算机基础-jyy ics"},{"content":"主要原理就是通过github action的来监听issue的相关事件，然后读取issue中的内容创建文件提交到git上，最后直接部署。这样就能随时编辑并展示了。\n具体可以看 \u0026nbsp;add event to myb log 这个仓库， 以及\u0026nbsp;使用方式 。\n接下来介绍如何开发一个github action\n1. 克隆官方提供的template 官方主要对typescript的支持比较好，提供了一系列的\u0026nbsp;工具包 ，没办法只能同ts来进行开发，\n\u0026nbsp;https://github.com/actions/typescript-action 2. 定义想要在运行时用户输入的参数 通过编辑action.yml这个文件来定义想要在运行时定义的参数，比如说GITHUB_TOKEN:\nname: \u0026#39;add event to my blog\u0026#39; description: \u0026#39;add event to my blog\u0026#39; author: \u0026#39;fzdwx\u0026#39; branding: icon: \u0026#39;archive\u0026#39; color: \u0026#39;white\u0026#39; inputs: # 在这个key下面添加自定义参数 token: required: true description: \u0026#39;the repo PAT or GITHUB_TOKEN\u0026#39; runs: using: \u0026#39;node16\u0026#39; main: \u0026#39;dist/index.js\u0026#39; 参数有三个属性:\nrequired: 是否必须 description: 描述 default: 默认值 3. 实现想要的功能 详细可以查看我的\u0026nbsp;主要代码 。我的里面主要做了:\n根据当前issue number\u0026nbsp;获取该issue的内容 。 根据预定义的模板，\u0026nbsp;创建文件内容 。 提交到\u0026nbsp;git 上。 4. 发布到marketplace 需要勾选Publish this Action to the GitHub Marketplace，可能需要2fa认证，找一个github支持就ok了。\nFigure 1 成功release后就能在别的项目中使用了。\n","permalink":"https://fzdwx.github.io/posts/2022-10-15-about-github-action/","summary":"起因是因为想要有一种可以不用编辑文件而作用到网站上的方式，然后就了解到了github action的形式。","title":"写一个自己的github action"},{"content":"起因 昨天晚上想用fzf与cd联动，就是fzf的结果传递给cd来执行于是有了这么一条命令:\ncd $(fd --type d | fzf) 这个命令也确实能完成任务，但是问题有两个:\n如果直接退出的话会回到家目录，因为$(..)的执行结果为空 每次都要输入这么多会很麻烦 用alias 然后尝试用alias来试试，所以就往.zshrc里面添加:\nalias cdf=\u0026#34;cd $(fd --type d | fzf)\u0026#34; 结果是直接不能运行，因为它直接识别了$(..)这一段，然后直接运行了，但是后面就不会运行。\n用shell脚本 然后就写了这个文件:\n#!/bin/sh path=$(fd --type d --strip-cwd-prefix --hidden --follow --exclude .git --exclude node_modules | fzf) if [ -z \u0026#34;$path\u0026#34; ]; then exit fi cd \u0026#34;$path\u0026#34; || exit 结果也是不行，后面我在最下面加了一行echo \u0026quot;$PWD\u0026quot;，我看到是执行了的，但是程序退出了就失效了。\n解决 最后我搜索到可以使用source xxx或者. xxx来解决，最后是alias+shell脚本来完成这个操作的:\nalias cdf=\u0026#34;source /path/to/cdf\u0026#34; 同时它也解决我上面提到的两个问题。\nsource为什么能解决？ 之所以直接用shell脚本直接运行会不行，是因为它不是在当前shell环境中运行的，而是一个子shell，所以结果就不能改变当前的文件目录了。\n而source或者.就代表着在当前的shell环境中执行，所以就能成功。\n","permalink":"https://fzdwx.github.io/posts/2022-10-11-about-source/","summary":"起因 昨天晚上想用fzf与cd联动，就是fzf的结果传递给cd来执行于是有了这么一条命令:\ncd $(fd --type d | fzf) 这个命令也确实能完成任务，但是问题有两个:\n如果直接退出的话会回到家目录，因为$(..)的执行结果为空 每次都要输入这么多会很麻烦 用alias 然后尝试用alias来试试，所以就往.zshrc里面添加:\nalias cdf=\u0026#34;cd $(fd --type d | fzf)\u0026#34; 结果是直接不能运行，因为它直接识别了$(..)这一段，然后直接运行了，但是后面就不会运行。\n用shell脚本 然后就写了这个文件:\n#!/bin/sh path=$(fd --type d --strip-cwd-prefix --hidden --follow --exclude .git --exclude node_modules | fzf) if [ -z \u0026#34;$path\u0026#34; ]; then exit fi cd \u0026#34;$path\u0026#34; || exit 结果也是不行，后面我在最下面加了一行echo \u0026quot;$PWD\u0026quot;，我看到是执行了的，但是程序退出了就失效了。\n解决 最后我搜索到可以使用source xxx或者. xxx来解决，最后是alias+shell脚本来完成这个操作的:\nalias cdf=\u0026#34;source /path/to/cdf\u0026#34; 同时它也解决我上面提到的两个问题。\nsource为什么能解决？ 之所以直接用shell脚本直接运行会不行，是因为它不是在当前shell环境中运行的，而是一个子shell，所以结果就不能改变当前的文件目录了。\n而source或者.就代表着在当前的shell环境中执行，所以就能成功。","title":"在shell脚本中执行cd后改变main shell的路径"},{"content":"idea 目前有一个想法，是在命令行下管理脚本的工具。\n例如说我有一些常用的脚本:\ncd $(find . -name \u0026#34;*\u0026#34; -type d | fzf) 然后通过命令行添加\ncli load \u0026#34;cd $(find . -name \u0026#34;*\u0026#34; -type d | fzf)\u0026#34; -alias cdf 然后使用cdf进行运行\ncli cdf 后续 2022-10-18 22:47\n刚刚找到了一个跟我这个想法很契合的项目: \u0026nbsp;https://github.com/denisidoro/navi 。\n今天一晚上都在调研技术的可行性，但是都达不到我想要的效果。还是这个例子，cd $(find . -name \u0026quot;*\u0026quot; -type d | fzf)， 主要有两种思路:\n在运行程序的使用利用shell的tab键盘补全，直接替换成这段命令，然后运行。比如说程序叫qwe，在shell里面输入qwe cdf\u0026lt;TAB\u0026gt; ，然后就直接替换为上面的命令。我在go里面找到cobra这个包，它能动态补全命令，有点效果，但还不够，不能做到全部替换，遂搁置。 直接在程序里面运行这段命令，我试了之后还是不行，cd执行后没有生效，应该还是跟fork有关。 然后就到github里面找别人的实现，没想到找到了一个，但是试过之后还是不支持，但确实做的还不错。\n想要做成我想要的效果就是实现:\n在shell中补全能直接替换所有，而不是一段。 在程序中不用fork运行。 还需要在看看。\n","permalink":"https://fzdwx.github.io/posts/2022-10-10-code-alias/","summary":"idea 目前有一个想法，是在命令行下管理脚本的工具。\n例如说我有一些常用的脚本:\ncd $(find . -name \u0026#34;*\u0026#34; -type d | fzf) 然后通过命令行添加\ncli load \u0026#34;cd $(find . -name \u0026#34;*\u0026#34; -type d | fzf)\u0026#34; -alias cdf 然后使用cdf进行运行\ncli cdf 后续 2022-10-18 22:47\n刚刚找到了一个跟我这个想法很契合的项目: \u0026nbsp;https://github.com/denisidoro/navi 。\n今天一晚上都在调研技术的可行性，但是都达不到我想要的效果。还是这个例子，cd $(find . -name \u0026quot;*\u0026quot; -type d | fzf)， 主要有两种思路:\n在运行程序的使用利用shell的tab键盘补全，直接替换成这段命令，然后运行。比如说程序叫qwe，在shell里面输入qwe cdf\u0026lt;TAB\u0026gt; ，然后就直接替换为上面的命令。我在go里面找到cobra这个包，它能动态补全命令，有点效果，但还不够，不能做到全部替换，遂搁置。 直接在程序里面运行这段命令，我试了之后还是不行，cd执行后没有生效，应该还是跟fork有关。 然后就到github里面找别人的实现，没想到找到了一个，但是试过之后还是不支持，但确实做的还不错。\n想要做成我想要的效果就是实现:\n在shell中补全能直接替换所有，而不是一段。 在程序中不用fork运行。 还需要在看看。","title":"Code:alias"},{"content":"Lab2文档翻译 由于我的英文不是很好，所以使用翻译软件进行翻译，然后人工进行校对进行理解。\n原文地址: \u0026nbsp;https://pdos.csail.mit.edu/6.824/labs/lab-raft.html Introduction 这是一系列实验中的第一个，我们将构建一个 fault-tolerant key/value storage system 。 在本实验中我们将实现 Raft (一种复制的状态机协议)。在下一个实验中，我们将在 Raft 上构建一个 key/value service 。 然后，您将在有多个副本的状态机上进行 shard(分片？根据 key 进行 hash 来决定路由到哪个副本上) 来提高性能。\n复制(replication)通过在多个复制服务器上存储其状态(即数据)的完整副本来实现 fault tolerance 。 即使有一些服务器出现 failure (崩溃或网络断开和抖动) replication 也允许它们继续运行。 挑战在于 failures 可能导致副本存在不同的数据。\nRaft 将客户端的请求组织成一个序列，被称为 log ，并且确保所有 replica servers 看到相同的 log 。 每个 replica 按照日志的顺序来执行客户端的请求，将它们应用于其本地的服务状态副本(就是运行来自客户端的命令)。 由于所有存活的副本读取的日志内容都是相同的，所以都以相同的顺序来执行请求，因此它们都有相同的服务状态。 如果一个服务器发生了 failure 但是后来又 recovery (恢复) 了，Raft 会负责将它的 log 更新到最新状态。只要至少大多数的服务器还活着，并且能够继续通信， 那么 Raft 将持续运行。如果没有到达这个数量，那么 Raft 将会停止运行，直到达到这个数量才会重新开始运行。\n在本 lab 中，你将把 Raft 实现为一个带有相关方法的 GO 的对象类型，目的是为了能在更大的模块中使用。 一组 Raft 实例通过 RPC 来维护 replicated logs。你的 Raft 实例将支持一连串不确定编号的 command， 也可以叫 log entries。 这些 entity 通过 index(索引)来进行编号。具有给定索引的 log entry 将被 commit， 此时，您的 Raft 应该将这条 log 发送到 larger service 上执行。\n你应该遵循\u0026nbsp;extended Raft paper 中设计， 特别是\u0026nbsp;图2 .你将实现论文中的大部分内容，包括保存持久化状态和节点故障自动重启后读取状态。 你将不会实现集群成员的变化(Section 6)。\n你可能会发现这个\u0026nbsp;指南 很有用， 还有这个关于concurrency的\u0026nbsp;锁 和\u0026nbsp;结构 的建议， 如果需要更广泛的视角，可以看看 Paxos,Chubby,Paxos Made Live,Spanner,Zookeeper,Harp,Viewstamped Replication 和\u0026nbsp;Bolosky et al 。\n请记住，本 lab 中最具挑战性的部分可能不是实现你的解决方案，而是调试它。为了帮助应对这一挑战，你可能需要把事件花在如何使你的实现更容易调试。 你可以参考\u0026nbsp;指导页 和这篇关于有效打印声明的\u0026nbsp;博文 。\n我们还提供了\u0026nbsp;Raft 交互图 ， 可以帮助阐明Raft代码如何与上层(使用者?)交互。\nThe code 通过向raft/raft.go添加代码来实现Raft。在该文件中，你会发现骨架代码，以及如何发送和接收 RPC 的例子。 你的实现必须支持以下接口，测试者和(最终)你的 key/value service 将使用该接口。你可以在raft.go的注释中找到更多细节。\nTIP Raft 实例只能通过 rpc 进行通信且必须使用labrpc这个包(例如不能使用文件以及共享变量)。 // create a new Raft server instance: rf := Make(peers, me, persister, applyCh) // start agreement on a new log entry: rf.Start(command interface{}) (index, term, isleader) // ask a Raft for its current term, and whether it thinks it is leader rf.GetState() (term, isLeader) // each time a new entry is committed to the log, each Raft peer // should send an ApplyMsg to the service (or tester). type ApplyMsg Make Make(peers []*labrpc.ClientEnd, me int,persister *Persister, applyCh chan ApplyMsg)\n用于创建 Raft server。\n所有的 raft server 的端口都在peers[]存放(包括当前的服务)，当前服务的端口可以通过peers[me]来获取。 所有的服务的perrs[]数组都具有相同的顺序。 presister是一个用来存放persistent state的地方，并且在初始的时候会保存最具的状态，如果有。 applyCh是 service 或 tester 发送消息给 raft 的通道。Make() 必须快速返回，所以它应该为一些长时间运行的任务启动goruntines。 Start 使用 Raft 的服务（例如 kv 服务器）希望就下一个要附加到 Raft 日志的命令开始协议。如果此服务器不是领导者，则返回 false。否则启动协议并立即返回。无法保证此命令将永远提交到 Raft 日志，因为领导者可能会失败或失去选举。即使 Raft 实例被杀死，这个函数也应该优雅地返回。第一个返回值是该命令在提交时将出现的索引。第二个返回值是当前术语。如果此服务器认为它是领导者，则第三个返回值为 true。\nStart(command interface{}) (int, int, bool)\n使用 Raft 的服务(e.g k/v server)希望就下一个要追加到 Raft 日志的命令开始协议。 如果当前 Raft server 不是 leader 则返回false。否则启动协议并立即返回，无需等待日志追加完成。 所以无法保证此命令将一定会被提交到 Raft 日志中，因为 leader 可能会失败或者在输掉选举。 即使 Raft 实例被 kill 这个函数也应该 return gracefully(优雅返回)。\n第一个返回值是该命令在 commit 时将被设置的 index。第二个返回值是当前的 term(任期)。如果此服务器认为自己是 leader 则第三个返回值是true。\n每个新提交的Raft log entity都应该发送一个AppliMsg到Make()的applyCh中。\n2A 实现 Raft leader election 以及 heartbeats(AppendEntries RPCs中不附带 log entries)。\n2A的目标是: 选出一个 leader，如果没有 failure，它仍然是 leader，如果 old leader 失败或者与 old leader 之间的数据包发生丢失则由 new leader 接管。\nTIP 这个失败是 leader 出现故障的意思？就是说只要它没出现运行故障或者网络问题就永远是leader？ 要点:\n通过运行go test -run 2A来进行测试你的实现。 按照论文的\u0026nbsp;图2 ，主要关心发送和接收RequestVote RPCs ，与the Rules for Servers that relate to elections 以及the State related to leader election。 添加\u0026nbsp;图2 中与 leader election 相关的状态到Raft这个结构体中，且还需要定义一个结构来保存每个日志的信息。 实现RequestVote()，这样 raft 服务们就能互相投票了。添加RequestVOteArgs和RequestVoteReply者两个结构体。修改Make() ，创建一个 goroutine，用于检查心跳消息，如果有一段时间没有收到 peer 的消息时将发送RequestVote RPCs来定期发起领导者选举。这样，如果有 leader 了，peer 将知道谁是 leader，或者自己成为 leader。 实现心跳，需要定义一个AppendEntries RPC结构(尽管你可能还不需要所有参数)， 并且让 leader 定期发送它。编写一个AppendEntries RPC的 handle method，用于重置选举超时， 这样当有一个人已经当选时，其他服务器不会又成为 leader。 确保不同 peer 的选举超时不在同一时间发生，否则所有peer将只为自己投票，这样就没有人会成为leader了。 在测试时，leader 每秒发送的RPC请求不能超过 10 次。 在测试时，要求 Raft 在 old leader 失败后5秒内选举 new leader(如果大多数节点仍然能继续通讯)。 但是请记住，如果出现split vote(如果数据包丢失或者候选人选择了相同的随机退避时间就有可能发生)，leader 选举可能需要多轮。 所以必须设置足够短的选举超时(也就是心跳间隔)，即使会选举多轮，也有可能在5秒内完成。 论文的\u0026nbsp;Leader election 这一节中提到的选举超时范围是150到300毫秒。 只有当 leader 发送心跳的频率大大高于150毫秒一次时，上面提到的范围才有意义。由于在测试时限制每秒10次心跳， 所以必须使用比论文中更大的选举超时时间，但是不能太大，因为可能会无法在5秒内完成选举。 如果您的代码无法通过测试，请再次阅读论文中的\u0026nbsp;图2 ，leader 选举的全部逻辑分布在图中多个部分。 不要忘记实现GetState()。 在测试时，如果要关闭一个raft实例，会调用rf.kill()。我们可以调用rf.killed来检查是否被调用了kill()。您可能希望在所有的循环中都这样 做，以避免死亡的Raft实例打印混乱的信息。 go RPC只发送名称以大写字母开头的结构体字段。子结构体也必须拥有大写的字段名。 Raft论文翻译 选取一些重要的片段进行翻译\n原文地址: \u0026nbsp;https://pdos.csail.mit.edu/6.824/papers/raft-extended.pdf Abstract Raft 是一种为了管理复制日志的共识算法。它提供了和 Paxos 算法相同的功能和性能，但它的算法结构和 Paxos 不同， Raft 更容易理解并且更容易构建实际的系统。为了提升可理解性，Raft 将共识算法分成了几个关键的模块，例如领导人选举，日志复制和安全性。 同时它通过实施一个更强的一致性来减少需要考虑的状态的数量。Raft 还包括一个机制来允许集群成员的动态改变，它利用重叠的大多数来保证安全性。\n原文 Raft is a consensus algorithm for managing a replicated log. It produces a result equivalent to (multi-)Paxos, and it is as efficient as Paxos, but its structure is different from Paxos; this makes Raft more understandable than Paxos and also provides a better foundation for building practical systems. In order to enhance understandability, Raft separates the key elements of consensus, such as leader election, log replication, and safety, and it enforces a stronger degree of coherency to reduce the number of states that must be considered. Results from a user study demonstrate that Raft is easier for students to learn than Paxos. Raft also includes a new mechanism for changing the cluster membership, which uses overlapping majorities to guarantee safety. Introduction Raft 算法和已经存在的共识算法在某些地方很相似(主要是 Oki 以及 Liskov\u0026rsquo;s Viewstamped Replication)，但是它有以下新特性:\n原文 raft is similar in many ways to existing consensus algorithms (most notably, Oki and Liskov’s Viewstamped Replication), but it has several novel features: 强领导者: Raft 使用一种比其他共识算法更强的领导形式。例如，日志只从 leader 发送给其他服务器。 这简化了对复制日志的管理并使得 Raft 更容易理解。 领导选举: Raft 使用随机的计时器来选取 leader。这种方式仅仅是在所有共识算法都需要改进的心跳机制上有些许改进，然而这使得 Raft 在解决冲突时更简单和快速。 成员调整: Raft 使用了新的联合共识(join consensus)算法来处理集群成员变换的问， 在处于调整过程中的两种不同的配置的大多数集群会有重叠(overlap)，这就允许集群在成员变更的时候，持续正常运行。 原文 Strong leader: Raft uses a stronger form of leadership than other consensus algorithms. For example,log entries only flow from the leader to other servers. This simplifies the management of the replicated log and makes Raft easier to understand. Leader election: Raft uses randomized timers to elect leaders. This adds only a small amount of mechanism to the heartbeats already required for any consensus algorithm, while resolving conflicts simply and rapidly. Membership changes: Raft’s mechanism for changing the set of servers in the cluster uses a new joint consensus approach where the majorities of two different configurations overlap during transitions. This allows the cluster to continue operating normally during configuration changes. Replicated State Machine Replicated State Machine(复制状态机)在分布式系统中被用于解决各种容错问题。例如 GFS、HDFS、RAMCloud 等单 leader 的大型集群系统， 通常使用独立的复制状态机来管理领导选举和存储配置信息来保证在 leader 崩溃的情况下也要存活下来，复制状态机的例子包括 Chubby 以及 Zookeeper。\nFigure 1: 复制状态机架构。共识算法管理来自客户端的包含状态机命令的复制日志，状态机按照相同的顺序来处理它们，所以它们产生相同的输出。 共识算法是在复制状态机的背景下提出的，在这种方法中，在一组服务器上的状态机对同一个的状态会计算出相同的副本，并且在一些服务器宕机的情况下也可以继续运行。\n原文 Replicated state machines are used to solve a variety of fault tolerance problems in distributed systems. For example, large-scale systems that have a single cluster leader, such as GFS, HDFS, and RAMCloud, typically use a separate replicated state machine to manage leader election and store configuration information that must survive leader crashes. Examples of replicated state machines include Chubby and ZooKeeper. Consensus algorithms typically arise in the context of replicated state machines.In this approach, state machines on a collection of servers compute identical copies of the same state and can continue operating even if some of the servers are down. 如图一所示，复制状态机是通过复制日志实现的。每个服务器保存者一个包含一系列命令的日志，其状态机按照顺序来执行它们。 每个日志包含相同顺序的相同命令，所以每个状态机都执行相同的命令序列。因为状态机是确定的，所以每个状态机会计算出相同的状态和相同顺序的输出。\n原文 Replicated state machines are typically implemented using a replicated log, as shown in Figure 1. Each server stores a log containing a series of commands, which its state machine executes in order. Each log contains the same commands in the same order, so each state machine processes the same sequence of commands. Since the state machines are deterministic, each computes the same state and the same sequence of outputs. 共识算法的任务是 保证复制日志的一致性 。服务器上的共识模块接收来自客户端的命令并把它们添加到日志中， 并与其他服务器上的共识模块进行通讯以确保它们的每一条日志最终都相同(相同的请求有相同的顺序)，即使有一些服务发生故障。 一旦命令被正确的复制，每一个服务的状态机会按照日志的顺序去处理它们，然后将结果返回给客户端。\n因此，这些服务似乎成为了一个单一的，高度可靠的状态机。\n原文 Keeping the replicated log consistent is the job of the consensus algorithm. The consensus module on a server receives commands from clients and adds them to its log. It communicates with the consensus modules on other servers to ensure that every log eventually contains the same requests in the same order, even if some servers fail. Once commands are properly replicated, each server’s state machine processes them in log order, and the outputs are returned to clients. As a result, the servers appear to form a single, highly reliable state machine. 在实际的共识算法通常有以下属性:\n确保非拜占庭(non-Byzantine)条件下的安全性(永远不返回错误的结果)，包括网络延迟，分区以及网络数据包丢失、冗余、乱序。 只要大多数的服务都在运行并能相互通信且和客户端通信，它们就能发挥出全部的功能(可用性)。因此，一个5台服务的集群能容忍2台服务出现故障。 假定服务应为停机而出现故障，它们可能稍后会从stable storage中恢复状态并从新加入集群。 不依赖与时序来保证日志的一致性: 错误的时钟和极端的信息延迟延迟在最坏的情况下会导致可用性问题。 在一般情况下，一个命令的完成在于集群中的大多数对单轮远程调用作出响应，少数比较慢的服务不会影响系统的整体性能。 原文 Consensus algorithms for practical systems typically have the following properties:\nThey ensure safety (never returning an incorrect result) under all non-Byzantine conditions, including network delays, partitions, and packet loss, duplication, and reordering. They are fully functional (available) as long as any majority of the servers are operational and can communicate with each other and with clients. Thus, a typical cluster of five servers can tolerate the failure of any two servers. Servers are assumed to fail by stopping; they may later recover from state on stable storage and rejoin the cluster. They do not depend on timing to ensure the consistency of the logs: faulty clocks and extreme message delays can, at worst, cause availability problems. In the common case, a command can complete as soon as a majority of the cluster has responded to a single round of remote procedure calls; a minority of low servers need not impact overall system performance. The Raft consensus algorithm Raft就是用于管理上一解描述的复制日志的算法。\u0026nbsp;图2 是对该算法的精简型式的总结，\u0026nbsp;图3 列出来该算法的关键属性，接下来对这些部分进行逐一讨论。\n原文 Raft is an algorithm for managing a replicated log of the form described in Section 2. Figure 2 summarizes the algorithm in condensed form for reference, and Figure 3 lists key properties of the algorithm; the elements of these figures are discussed piecewise over the rest of this section. Raft 通过选出一位 leader 然后给予它完全管理日志复制的责任来实现共识。leader 接收来自客户端的日志，然后复制给其他服务，并且通知在何时 它们可以安全的消费(作用到状态机上)这些日志。leader 简化了日志复制的管理。例如: leader 可以自主确定新日志存放在哪个位置而不用询问其他服务， 并且数据都从 leader 流向其他服务器。leader 可能会发生故障，或者和其他服务器失去连接，这个时候需要重新选举 leader。\n原文 Raft implements consensus by first electing a distinguished leader, then giving the leader complete responsibility for managing the replicated log. The leader accepts log entries from clients, replicates them on other servers, and tells servers when it is safe to apply log entries to their state machines. Having a leader simplifies the management of the replicated log. For example, the leader can decide where to place new entries in the log without consulting other servers, and data flows in a simple fashion from the leader to other servers. A leader can fail or become disconnected from the other servers, in which case a new leader is elected. 基于 leader 的方法，Raft 将一致性问题为了三个子过程来解决:\nleader 选举: 当 leader 失败(宕机)时需要选举新 leader 日志复制: leader 接收来自客户端的日志，并复制给集群中的其他机器，强制其他服务器与自己的一致 安全: Raft 的安全就在于\u0026nbsp;图3 中的安全属性: 如果任何服务器消费了一个日志，那么其他任何服务器就不 能在相同的日志索引消费不同的日志 原文 Given the leader approach, Raft decomposes the consensus problem into three relatively independent subproblems, which are discussed in the subsections that follow:\nLeader election: a new leader must be chosen when an existing leader fails Log replication: the leader must accept log entries from clients and replicate them across the cluster, forcing the other logs to agree with its own Safety: the key safety property for Raft is the State Machine Safety Property in Figure 3: if any server has applied a particular log entry to its state machine, then no other server may apply a different command for the same log index. Section 5.4 describes how Raft ensures this property; the solution involves an additional restriction on the election mechanism described in Section 5.2. Figure 2 Figure 2: Raft共识算法的精简摘要(不包括成员更改以及日志压缩)。左上角的服务器行为被描述为一组独立且重复触发的规则。 state:\n在所有服务器上持久化(在响应 RPC 请求之前，已经更新到了稳定的存储设备) currentTerm: 服务器已知最新的任期（在服务器首次启动时初始化为0，单调递增） votedFor: 当前任期内收到选票的 candidate Id，如果没有投给任何候选人则为空 log[]: 日志条目,每个条目包含了用于状态机的命令，以及领导人接收到该条目时的任期(初始索引为1) 所有服务器上的易失性状态 commitIndex: 已知的提交的日志中的最大索引(初始0，单调递增) lastApplied: 状态机已经执行的日志中的最大索引(初始0，单调递增) 在 leader 上不稳定存在(在每次重新选举后初始化) nextIndex[]: 对于每一个服务器，发送到该服务器的下一个日志条目的索引(初始为leader的最后一条日志索引+1) matchIndex[]: 对于每一个服务器，已知的已经复制到该服务器的最高日志条目的索引(初始0，单调递增) AppendEntries RPC\n由leader发起调用来复制日志，同时也用于心跳检测\nArguments: term: leader 的任期 leaderId: 用于 follower 找到 leader(有时 client 把请求发送给了 follower) prevLogIndex: 紧邻新日志条目之前的那个日志条目的索引 prevLogTerm: 紧邻新日志条目之前的那个日志条目的任期 entries[]: 需要被保存的日志(为空时是心跳检测，可能一次会发送多条来提升效率) leaderCommit: leader 已知的该 follower 的commitIndex Results: term: currentTerm，用于 leader 更新自己的term success: 如果follower的pervLogIndex以及prevLogTerm能够匹配上则为true Receiver implementation: if term \u0026lt; currentTerm then return false(如果 leader 的任期小于接收者的当前任期，接收者可以是 follower 以及 candidate) if log[prevLogIndex].term != prevLogTerm then return false(在接收者日志中 如果能找到一个和 prevLogIndex 以及 prevLogTerm 一样的索引和任期的日志条目则继续执行下面的步骤否则返回假) if log[oldIndex].term != log[newIndex].term then remove log[oldIndex,lastIndex]( 如果一个已经存在的日志与新的日志(请求中的日志条目)冲突(索引相同，任期不同)，则删除该索引处以及之后的所有日志) 添加在日志列表中不存在的新日志 if leaderCommit \u0026gt; commitIndex then commitIndex=min(leaderCommit,log[].last.commitIndex)( 如果 leader 的已知已提交的最高日志条目的索引大于接收者的已知已提交最高日志条目的索引，则把接收者的已知已经提交的最高的日志条目的索引commitIndex 重置为 leader 的已知已经提交的最高的日志条目的索引 leaderCommit 或者是 上一个新条目的索引 取两者的最小值) RequestVote RPC\n候选人调用，收集选票\nArguments: term: candidate 的任期号 candidateId: 发起请求的 candidate 的 id lastLogIndex: candidate 的最后一条日志的索引 lastLogTerm: candidate 最后一条日志对应的任期号 Results: term: 当前任期号，用于candidate更新自己的term voteGranted: true表示候选人获得了选票 Receiver implementation: if term \u0026lt; currentTerm then return false(如果term \u0026lt; currentTerm返回 false) if (votedFor==null||votedFor==candidateId)\u0026amp;\u0026amp;(lastLogIndex,lastLogTerm)==log[].last then return true (如果 votedFor 为空或者与 candidateId 相同，并且候选人的日志和自己的日志一样新，则给该候选人投票) Rules for Servers\nAll Servers: if commitIndex \u0026gt; lastApplied then incr lastApplied and exec log[lastApplied]（如果commitIndex \u0026gt; lastApplied，lastApplied自增，将log[lastApplied]应用到状态机） if appendEntries.logs exist (log.term \u0026gt; currentTerm) then currentTerm = log.term and set status = follower(如果 RPC 的请求或者响应中包含一个 term T 大于 currentTerm，则 currentTerm 赋值为 T，并切换状态为追随者 follower) Followers: 不会发出任何请求，只会对来自 candidates 以及 leader 的请求做出响应 选举超时后，如果未收到当前 leader(term 相同)的AppendEntries RPC或投票给了 candidate，则转换为 candidate Candidates: 转换成candidate之后开始选举 incr currentTerm 投票给自己 reset election timer 发送RequestVote RPC给其他所有服务器 如果收到了多数的选票则成为 leader 如果收到 new leader 的AppendEntries RPC则成为 follower 如果选举超时则开始新一轮的选举 Leaders: 一旦成为 leader 则向其他服务器发送空的AppendEntries RPC，在空闲时重复发送以防止选举超时 如果收到来自 client 的命令: 添加到本地日志，在执行并作用到状态机后作出响应给 client 对于 follower if last log index \u0026gt;= nextIndex(如果上一次收到的日志的索引大于这次要发送给它的日志的索引( nextIndex)): 则通过AppendEntries RPC将nextIndex之后的所有日志都发送发送出去 如果成功: 将该 follower 的nextIndex以及matchIndex更新 如果因为日志不一致导致失败: nextIndex递减并重新发送 如果存在一个数N，满足N \u0026gt; commitIndex，大多数的matchIndex[i] \u0026gt;= N 以及log[N].term == currentTerm: set commitIndex = N Figure 3 Figure 3: Raft保证这些属性在在任何时候都上正确的。 Election Safety: 在给定 term 内只能选出一个 leader Leader Append-Only: leader 永远不覆盖或删除日志，只会添加 Log Matching: 如果两个日志在包含相同的 index 以及 term，那么就认定它们完全相同 Leader Completeness: 如果一条日志在给定的 term 内提交，那么它一定会出现在 term 更大的 leader 的日志中 State Machine Safety: 如果一个服务器已经将给定索引位置的日志条目应用到状态机之中，则其他所有服务器不会在相同索引处出现不同的日志 Raft basics 一个 Raft 集群可以包含多个服务器；5 是一个典型的数量，它允许系统容忍2次故障(有两台服务宕机)。 在给定的时间中每个服务都处在以下三种状态之一: leader, follower, candidate。 正常情况下，恰好只有一个 leader，所有其他服务器都是 follower。\nfollower 是被动的: 它们不会自己发出请求，而只是响应来自 leader 和 candidate 的请求。 leader 处理所有 client 的请求(如果 client 联系到 follower，则 follower 重定向到 leader)。 candidate 用于选举出一个新的 leader(可以看\u0026nbsp;图4 )。 Figure 4 Figure 4: Server states。follow 只响应其他服务的请求，如果 follow 接收不到任何消息，就会变成 candidate 并发起选举。获得整个集群 中大多数人投票的 candidate 成为候选人。leader 通常运行到它们失败为止。 原文 A Raft cluster contains several servers; five is a typical number, which allows the system to tolerate two failures. At any given time each server is in one of three states: leader, follower, or candidate. In normal operation there is exactly one leader and all of the other servers are followers. Followers are passive: they issue no requests on their own but simply respond to requests from leaders and candidates. The leader handles all client requests (if a client contacts a follower, the follower redirects it to the leader). The third state, candidate, is used to elect a new leader as described in Section 5.2. Figure 4 shows the states and their transitions; the transitions are discussed below. 如图5所示: Raft将时间分为任意长度的 terms。terms 的编号是连续的整数。每一个 term 开始于 election，一个或多个 candidate 尝试成为 leader。如果一个 candidate 赢得了选举，那么它将在剩下的 term 内担任 leader。\n在某些特殊情况下选举的结果是 split vote。在这种情况下，term 将会结束并且没有 leader。一个新的 term(伴随新一轮的选举)将很快开始。 Raft保证在给定的 term 内最多只有一个 leader。\nFigure 5 Figure 5: 将时间划分为 terms，每个 term 都以选举开始。选举成功后，一个 leader 管理集群直到 term 结束。 有时候选举会失败，那么这个 term 就会没有 leader 而结束。 term 之间的切换可以在不同的时间不同的服务器上观察到 原文 Raft divides time into terms of arbitrary length, as shown in Figure 5. Terms are numbered with consecutive integers. Each term begins with an election, in which one or more candidates attempt to become leader as described in Section 5.2. If a candidate wins the election, then it serves as leader for the rest of the term. In some situations an election will result in a split vote. In this case the term will end with no leader; a new term (with a new election) will begin shortly. Raft ensures that there is at most one leader in a given term. 不同的服务器可能会在不同的时间观察到 term 之间的转换，在某些情况下，一个服务器可能不会观察到选举甚至整个 term 的全程。 term 在 Raft 中充当了逻辑时钟，term 使得可以服务器检测过时的信息: 如过时的 leader。\n每个服务器都存储一个当前的 term 编号，该编号随时间单调地增加。每当服务器进行通信时，就会交换当前 term； 如果一个服务器的当前 term 比另一个服务器的小，那么它就会将其当前 term 更新为较大的值。\n如果一个 candidate 或 leader 发现它的 term 已经过时，它将立即恢复到 follower 的状态。\n如果一个服务器收到的请求是一个过时的 term 编号，它将拒绝该请求。\n原文 Different servers may observe the transitions between terms at different times, and in some situations a server may not observe an election or even entire terms. Terms act as a logical clock [14] in Raft, and they allow servers to detect obsolete information such as stale leaders. Each server stores a current term number, which increases monotonically over time. Current terms are exchanged whenever servers communicate; if one server’s current term is smaller than the other’s, then it updates its current term to the larger value. If a candidate or leader discovers that its term is out of date, it immediately reverts to follower state. If a server receives a request with a stale term number, it rejects the request. Raft 服务器使用 RPC 进行通信，而基本的共识算法只需要两种类型的RPC。RequestVote RPCs由 candidate 在选举期间发起； AppendEntries RPCs由 leader 发起，用于复制日志条目并提供一种心跳机制。在下面的章节还增加了第三个RPC，用于在服务器之间传输快照。 如果服务器没有及时收到响应，它们会重试 RPC，并且为了获得最佳性能，它们会并行地发出 RPC。\n原文 Raft servers communicate using remote procedure calls (RPCs), and the basic consensus algorithm requires only two types of RPCs. RequestVote RPCs are initiated by candidates during elections (Section 5.2), and AppendEntries RPCs are initiated by leaders to replicate log entries and to provide a form of heartbeat (Section 5.3). Section 7 adds a third RPC for transferring snapshots between servers. Servers retry RPCs if they do not receive a response in a timely manner, and they issue RPCs in parallel for best performance. Leader election Raft 使用心跳机制来触发 leader 选举。当服务器启动时，初始状态都是 follower 。只要服务器收到来自 leader 或 candidate 的有效RPC， 它就一直处于 follower 状态。 leader 定期向所有 follower 发送心跳（AppendEntries RPCs，不携带日志条目），以保持他们的权威。 如果 follower 在一段时间内没有收到任何通信(election timeout)，那么它就认为没有可用的 leader，并开始选举以选择一个新的 leader。\n原文 Raft uses a heartbeat mechanism to trigger leader election. When servers start up, they begin as followers. A server remains in follower state as long as it receives validRPCs from a leader or candidate. Leaders send periodic heartbeats (AppendEntries RPCs that carry no log entries) to all followers in order to maintain their authority. If a follower receives no communication over a period of time called the election timeout, then it assumes there is no viable leader and begins an election to choose a new leader. 要开始一场选举，follower 要增加它的当前 term 并过转换到 candidate 状态。 然后，它为自己投票，并行的向集群中的每个其他服务器发出RequestVote RPCs。 candidate 将一直处于这种状态，直到发生以下三种情况之一:\n它赢得了选举 另一个服务器确立了自己的领导地位 一段时间之后没有任何人获胜。 接下来就对这些结果进行讨论:\n原文 To begin an election, a follower increments its current term and transitions to candidate state. It then votes for itself and issues RequestVote RPCs in parallel to each of the other servers in the cluster. A candidate continues in this state until one of three things happens: (a) it wins the election, (b) another server establishes itself as leader, or (c) a period of time goes by with no winner. These outcomes are discussed separately in the paragraphs below 它赢得了选举\n如果一个 candidate 在同一任期( term )内获得了整个集群中大多数服务器的投票，那么它就赢得了选举。 每台服务器在给定的 term 内最多为一名 candidate 投票，以先来后到为原则。\n少数服从多数的原则保证了最多只有一名 candidate能够在某一 term 内赢得选举 (\u0026nbsp;图3 中的选举 Safety 属性)。 一旦一个 candidate 在选举中获胜，它就成为 leader。然后，它向所有其他服务器发送心跳信息(不携带日志的AppendEntries RPC)， 以建立其权威并防止新的选举发生。\n原文 A candidate wins an election if it receives votes from a majority of the servers in the full cluster for the same term. Each server will vote for at most one candidate in a given term, on a first-come-first-served basis (note: Section 5.4 adds an additional restriction on votes). The majority rule ensures that at most one candidate can win the election for a particular term (the Election Safety Property in Figure 3). Once a candidate wins an election, it becomes leader. It then sends heartbeat messages to all of the other servers to establish its authority and prevent new elections. 另一个服务器确立了自己的领导地位\n在等待投票的过程中，candidate 可能会收到另一个服务器的AppendEntries RPC，声称自己是领导者。 如果这个 leader 的term(会携带在 RPC 中)不小于 candidate 当前的 term， 那么 candidate 就会承认 leader 是合法的并返回到 follower 状态。 如果 RPC 中的 term 比 candidate 当前的 term 小，那么候选者会拒绝 RPC，并继续处于 candidate 状态。\n原文 While waiting for votes, a candidate may receive an AppendEntries RPC from another server claiming to be leader. If the leader’s term (included in its RPC) is at least as large as the candidate’s current term, then the candidate recognizes the leader as legitimate and returns to follower state. If the term in the RPC is smaller than the candidate’s current term, then the candidate rejects the RPC and continues in candidate state. 一段时间之后没有任何人获胜\n第三个可能的结果是 candidate 既没有赢得选举，也没有输掉选举: 如果许多 follower 同时成为 candidate，那么票数可能被分割， 因此没有 candidate 能获得足够的投票。 当这种情况发生时，每个 candidate 都会超时，然后通过增加其 term 和发起新一轮的RequestVote RPC来开始新的选举。 然而，如果没有额外的措施，split vote可能会无限期地重复。\n原文 The third possible outcome is that a candidate neither wins nor loses the election: if many followers become candidates at the same time, votes could be split so that no candidate obtains a majority. When this happens, each candidate will time out and start a new election by incrementing its term and initiating another round of RequestVote RPCs. However, without extra measures split votes could repeat indefinitely. Raft 使用随机的选举超时时间，以确保 split vote 很少发生，并能迅速解决。 为了从一开始就防止 split vote，选举超时时间是从一个固定的时间间隔(例如150-300ms)中随机选择的。 这样每个服务器的选举超时时间就不同了，所以在大多数情况下，只有一个服务器会超时。\n如果一个服务赢得了选举，就在其他服务超时之前发送心跳，split vote就是使用这样的机制来处理。 每个 candidate 在选举开始时重新启动其随机选举超时(重新计时？)，并等待超时过后再开始下一次选举； 这减少了在新的选举中再次出现分裂票的可能性。\n原文 Raft uses randomized election timeouts to ensure that split votes are rare and that they are resolved quickly. To prevent split votes in the first place, election timeouts are chosen randomly from a fixed interval (e.g., 150–300ms). This spreads out the servers so that in most cases only a single server will time out; it wins the election and sends heartbeats before any other servers time out. The same mechanism is used to handle split votes. Each candidate restarts its randomized election timeout at the start of an election, and it waits for that timeout to elapse before starting the next election; this reduces the likelihood of another split vote in the new election. Section 9.3 shows that this approach elects a leader rapidly 选举是一个用于说明可理解性是如何指导我们在设计方案做权衡的例子。 最初我们计划使用一个排名系统: 每个 candidate 被分配一个唯一的排名，用来在竞争的 candidate 之间进行选择。 如果一个候选人发现了另一个排名更高的候选人，它就会回到 follower 的状态，这样排名更高的候选人就能更容易地赢得下一次选举。 我们发现这种方法在可用性方面产生了一些微妙的问题(如果一个排名较高的服务发生故障了，一个排名较低的服务器可能需超时并再次成为 candidate ，但如果它过早地这样做，它可能会重置选举 leader 的进展)。我们对算法进行了多次调整，但每次调整后都会出现新的角落案例。 最终我们得出结论，随机重试的方法更明显，更容易理解。\n原文 Elections are an example of how understandability guided our choice between design alternatives. Initially we planned to use a ranking system: each candidate was assigned a unique rank, which was used to select between competing candidates. If a candidate discovered another candidate with higher rank, it would return to follower state so that the higher ranking candidate could more easily win the next election. We found that this approach created subtle issues around availability (a lower-ranked server might need to time out and become a candidate again if a higher-ranked server fails, but if it does so too soon, it can reset progress towards electing a leader). We made adjustments to the algorithm several times, but after each adjustment new corner cases appeared. Eventually we concluded that the randomized retry approach is more obvious and understandable. Log replication 一旦一个领导者被选出，它就开始为 client 的请求提供服务。每个 client 的请求都包含一个要由复制的状态机执行的 command。 leader 将该 command 作为一个新的条目附加到它的日志中，然后并行发起AppendEntries RPC给其他每个服务器以复制该条日志。 当条目被安全复制后(如下所述)，leader 将这条日志应用于其状态机，并将执行结果返回给 client。 如果 follower 崩溃或运行缓慢，又或者网络数据包丢失，leader 会无限期地重试AppendEntries RPC(甚至在它回应了客户端之后)， 直到所有 follower 最终存储所有日志条目。\n原文 Once a leader has been elected, it begins servicing client requests. Each client request contains a command to be executed by the replicated state machines. The leader appends the command to its log as a new entry, then is- sues AppendEntries RPCs in parallel to each of the other servers to replicate the entry. When the entry has been safely replicated (as described below), the leader applies the entry to its state machine and returns the result of that execution to the client. If followers crash or run slowly, or if network packets are lost, the leader retries Append- Entries RPCs indefinitely (even after it has responded to the client) until all followers eventually store all log en- tries. Figure 6: 日志由有序序号标记的条目组成。每个条目都包含创建它的 term(每个框中的数字)和一个状态机需要执行的命令。 如果一个条目可以安全地应用于状态机，那么该条目就被认为是 committed 的 日志的组织方式如\u0026nbsp;图6 所示。每个日志条目都存储了一个状态机命令， 以及 leader 收到该条目时的 term 编号。日志条目中的 term 编号被用来检测日志之间的不一致， 并确保\u0026nbsp;图3 中的一些属性。每个日志条目也有一个整数的索引来标识它在日志中的位置。\n原文 Logs are organized as shown in Figure 6. Each log entry stores a state machine command along with the term number when the entry was received by the leader. The term numbers in log entries are used to detect inconsistencies between logs and to ensure some of the properties in Figure 3. Each log entry also has an integer index identifying its position in the log. leader 决定何时将日志条目应用于状态机是安全的，这样的条目被称为 committed 。 Raft 保证所提交的条目是持久化的并且最终会被所有可用的状态机执行。一旦创建该条目的 leader 将其复制到大多数服务器上， 该日志条目就会被提交(例如，图6中的条目7)。这也会提交 leader 日志中所有之前的条目，包括之前领导者创建的条目。 第5.4节讨论了在 leader 变更后应用这一规则时的一些微妙之处，它还表明这种承诺的定义是安全的。 leader 会跟踪它所知道的已提交的最大索引，并且它在未来的AppendEntries RPC(包括心跳)中包括该索引，以便其他服务器最终发现。 一旦 follower 得知一个日志条目被提交，它就会将该条目应用于其本地状态机(按日志顺序)。\n原文 The leader decides when it is safe to apply a log entry to the state machines; such an entry is called committed. Raft guarantees that committed entries are durable and will eventually be executed by all of the available state machines. A log entry is committed once the leader that created the entry has replicated it on a majority of the servers (e.g., entry 7 in Figure 6). This also commits all preceding entries in the leader’s log, including entries created by previous leaders. Section 5.4 discusses some subtleties when applying this rule after leader changes, and it also shows that this definition of commitment is safe. The leader keeps track of the highest index it knows to be committed, and it includes that index in future AppendEntries RPCs (including heartbeats) so that the other servers eventually find out. Once a follower learns that a log entry is committed, it applies the entry to its local state machine (in log order) 我们设计的 Raft 日志机制在不同服务器上的日志之间保持高度的一致性。这不仅简化了系统的行为，使其更具可预测性，而且是确保安全的重要组成部分。 Raft 维护了以下特性，它们共同构成了\u0026nbsp;图3 中的Log Matching特性:\n如果不同的两个日志具有相同的 index 以及 term\n那么就认为它们存储的是同一个 command 那么就认为它们之前的所有日志也是相同的 原文 We designed the Raft log mechanism to maintain a high level of coherency between the logs on different servers. Not only does this simplify the system’s behavior and make it more predictable, but it is an important component of ensuring safety. Raft maintains the following properties, which together constitute the Log Matching Property in Figure 3:\nIf two entries in different logs have the same index and term, then they store the same command. If two entries in different logs have the same index and term, then the logs are identical in all preceding entries 第一个属性来自于这样一个事实，即一个 leader 在一个 term 内只能在一个 index 上创建一个日志条目，并且日志条目永远不会改变它们在日志中的位置。 第二个属性由AppendEntries RPC执行的简单一致性检查来保证。 当发送AppendEntries RPC时，leader 会包含其日志中紧接新条目之前的条目的 index 和 term 。 如果 follower 在其日志中没有找到具有相同 index 和 term 的条目，那么它将拒绝新条目。 一致性检查就像一个归纳步骤: 日志的初始状态是肯定满足Log Matching属性的， 并且每当日志被扩展时，一致性检查都会保留Log Matching属性。 因此，每当AppendEntries成功返回时，leader 知道 follower 的日志与自己的日志在新条目之前是相同的\n原文 The first property follows from the fact that a leader creates at most one entry with a given log index in a given term, and log entries never change their position in the log. The second property is guaranteed by a simple consistency check performed by AppendEntries. When sending an AppendEntries RPC, the leader includes the index and term of the entry in its log that immediately precedes the new entries. If the follower does not find an entry in its log with the same index and term, then it refuses the new entries. The consistency check acts as an induction step: the initial empty state of the logs satisfies the Log Matching Property, and the consistency check preserves the Log Matching Property whenever logs are extended. As a result, whenever AppendEntries returns successfully, the leader knows that the follower’s log is identical to its own log up through the new entries 在正常运行期间，leader 和 follower 的日志保持一致，所以AppendEntries一致性检查不会失败。 然而，leader 崩溃会使日志不一致(old leader 可能没有完全复制其日志中的所有条目)。 这些不一致会在一系列 leader 和 follower 的崩溃中加剧。图7说明了 follower 的日志可能与new leader 的日志不同的方式。\nfollower 可能会丢失 leader 的条目 follower 可能会有 leader 没有的额外条目 或者两者都有 日志中缺失和多余的条目可能跨越多个 term 。\nFigure 7: 当一个 leader 成功掌权时，follower 可能是(a-f)中的任何一种情况。 每个盒子代表一个日志条目；盒子里的数字是其 term。一个 follower 可能缺少条目(a-b)，可能有额外的未承诺的条目(c-d)，或者两者都有(e-f)。 例如f的发生条件: 如果该服务器是第2期的 leader ，在其日志中中已经增加了几个条目，然后在提交条目之前就崩溃了；它很快重新启动，成为第3期的 leader， 并在其日志中增加了几个条目；在第2期或第3期的条目被提交之前，该服务器又崩溃了，并持续了几个任期。 原文 During normal operation, the logs of the leader and followers stay consistent, so the AppendEntries consistency check never fails. However, leader crashes can leave the logs inconsistent (the old leader may not have fully replicated all of the entries in its log). These inconsistencies can compound over a series of leader and follower crashes. Figure 7 illustrates the ways in which followers’ logs may differ from that of a new leader. A follower may be missing entries that are present on the leader, it may have extra entries that are not present on the leader, or both. Missing and extraneous entries in a log may span multiple terms. 在 Raft 中，leader 通过强迫 follower 的日志重复自己的日志来处理不一致的情况。这意味着 follower 日志中的冲突条目将被 leader 日志中的条目覆盖。在下一节将表明，如果再加上一个限制，这就是安全的。\n原文 In Raft, the leader handles inconsistencies by forcing the followers’ logs to duplicate its own. This means that conflicting entries in follower logs will be overwritten with entries from the leader’s log. Section 5.4 will show that this is safe when coupled with one more restriction. 为了使 follower 的日志与自己的日志保持一致，leader 必须找到两个日志一致的最新日志条目，删除该点之后 follower 日志中的所有条目， 并将该点之后的所有 leader 条目发送给 follower。所有这些操作都是为了响应AppendEntries RPC执行的一致性检查而发生的。 leader 为每个 follower 维护一个 nextIndex ，这是 leader 将发送给该 follower 的下一个日志条目的 index 。 当 leader 当选时，它会初始化所有 nextIndex 的值为自己最后一条日志的 index + 1(图 7 中的 11)。 如果 follower 的日志与 leader 的日志不一致，则AppendEntries一致性检查将在下一个AppendEntries RPC中失败。 follower 拒绝后，leader 会减少 nextIndex 的值并重试AppendEntries RPC。 最终nextIndex将达到 leader 和 follower 日志匹配的点。 发生这种情况时，AppendEntries将成功，这将删除 follower 日志中的任何冲突条目，并从 leader 日志中添加条目(如果有)。 一旦AppendEntries成功，follower 的 log 就会和 leader 的一致，并且在接下来的任期内保持这种状态。\n原文 To bring a follower’s log into consistency with its own, the leader must find the latest log entry where the two logs agree, delete any entries in the follower’s log after that point, and send the follower all of the leader’s entries after that point. All of these actions happen in response to the consistency check performed by AppendEntries RPCs. The leader maintains a nextIndex for each follower, which is the index of the next log entry the leader will send to that follower. When a leader first comes to power, it initializes all nextIndex values to the index just after the last one in its log (11 in Figure 7). If a follower’s log is inconsistent with the leader’s, the AppendEntries consis- tency check will fail in the next AppendEntries RPC. Af- ter a rejection, the leader decrements nextIndex and retries the AppendEntries RPC. Eventually nextIndex will reach a point where the leader and follower logs match. When this happens, AppendEntries will succeed, which removes any conflicting entries in the follower’s log and appends entries from the leader’s log (if any). Once AppendEntries succeeds, the follower’s log is consistent with the leader’s, and it will remain that way for the rest of the term 如果需要，可以优化协议以减少被拒绝的AppendEntries RPC的数量。例如，当拒绝AppendEntries请求时， follower 可以包含冲突条目的 term 以它在 term 中存储的第一个索引。 有了这些信息，leader 可以减少 nextIndex 以绕过该 term 中的所有冲突条目； 每个有日志冲突的 term 都只需要一个AppendEntries RPC，而不是每个日志条目一个 RPC。 在实践中，我们怀疑这种优化是否必要，因为失败很少发生，而且不太可能有很多不一致的条目。\n原文 If desired, the protocol can be optimized to reduce the number of rejected AppendEntries RPCs. For example, when rejecting an AppendEntries request, the follower can include the term of the conflicting entry and the first index it stores for that term. With this information, the leader can decrement nextIndex to bypass all of the conflicting entries in that term; one AppendEntries RPC will be required for each term with conflicting entries, rather than one RPC per entry. In practice, we doubt this optimization is necessary, since failures happen infrequently and it is unlikely that there will be many inconsistent en- tries 通过这种机制，leader 在上台时无需采取任何特殊措施来恢复日志一致性。它刚刚开始正常运行， 并且日志会自动收敛以响应AppendEntries一致性检查的失败。 leader 永远不会覆盖或删除自己日志中的条目(\u0026nbsp;图3 中的Leader Append-Only)。\n原文 With this mechanism, a leader does not need to take any special actions to restore log consistency when it comes to power. It just begins normal operation, and the logs automatically converge in response to failures of the Append- Entries consistency check. A leader never overwrites or deletes entries in its own log (the Leader Append-Only Property in Figure 3). 理想的 Raft:\n只要大多数服务器启动，Raft 就可以接受、复制和应用新的日志条目 可以通过单轮 RPC 将新条目复制到集群的大部分； 并且单个慢速 follower 不会影响性能。 原文 This log replication mechanism exhibits the desirable consensus properties described in Section 2: Raft can ac- cept, replicate, and apply new log entries as long as a ma- jority of the servers are up; in the normal case a new entry can be replicated with a single round of RPCs to a ma- jority of the cluster; and a single slow follower will not impact performance. Safety 在前面的章节里面介绍了 Raft 怎样选举领导以及复制日志。 然而，这些机制还远不够来保证每个状态机以相同的顺序来执行相同的 command。 例如，一个 follower 可能是不可用状态(unavailable)而 leader 提交了若干个日志，然后它可能会被选为 leader 然后覆盖这些日志； 结果就是不同的状态机可能执行了不同的 command sequence。\n原文 The previous sections described how Raft elects leaders and replicates log entries. However, the mechanisms described so far are not quite sufficient to ensure that each state machine executes exactly the same commands in the same order. For example, a follower might be unavailable while the leader commits several log entries, then it could be elected leader and overwrite these entries with new ones; as a result, different state machines might execute different command sequences. 在这一节通过添加对哪些服务器可以被选举为 leader的限制来完善 Raft。 这个限制确保 leader 在任意给定 term 内，包含了之前任职期间的所有被提交的日志(\u0026nbsp;图3 中的Leader Completeness Property) 增加这个选举限制，让我们使提交时的规则更加准确。 最后，我们会展示一个简要的证明为Leader Completeness Property并且说明是怎样引导出正确行为的复制状态机。\n原文 This section completes the Raft algorithm by adding a restriction on which servers may be elected leader. The restriction ensures that the leader for any given term contains all of the entries committed in previous terms (the Leader Completeness Property from Figure 3). Given the election restriction, we then make the rules for commitment more precise. Finally, we present a proof sketch for the Leader Completeness Property and show how it leads to correct behavior of the replicated state machine. Election restriction 在任何基于 leader 的共识算法中，leader 必须将已提交的日志存储。在一些共识算法中，比如Viewstamped Replication， 一个节点能当选 leader，即使它一开始没有包含所有已提交的日志。 这些算法都有额外的机制来识别丢失的日志并发送给 new leader，要么在选举的过程中或者在选举之后不久。 不幸的是，这种方法会导致相当大的额外的机制和复杂性。\nRaft 使用了一种更加简单的方法，它可以保证在选举的时候新的 leader 拥有所有之前任期中已经提交的日志条目， 而不需要传送这些日志条目给 leader 。这意味着日志条目的传送是单向的，只从 leader 传给 follower， 并且 leader 从不会覆盖自身本地日志中已经存在的条目。\n原文 In any leader-based consensus algorithm, the leader must eventually store all of the committed log entries. In some consensus algorithms, such as Viewstamped Replication [22], a leader can be elected even if it doesn’t initially contain all of the committed entries. These algorithms contain additional mechanisms to identify the missing entries and transmit them to the new leader, either during the election process or shortly afterwards. Unfortunately, this results in considerable additional mechanism and complexity. Raft uses a simpler approach where it guarantees that all the committed entries from previous terms are present on each new leader from the moment of its election, without the need to transfer those entries to the leader. This means that log entries only flow in one direction, from leaders to followers, and leaders never overwrite existing entries in their logs Raft 使用投票的过程来阻止 candidate 赢得选举，除非它的日志包含所有已经提交的条目。cand- idate 为了被选举必须联系集群中的大部分节点，这意味着每个被提交的日志在这些服务上至少存在一个节点上。 如果 candidate 的日志至少与大多数日志中的任何其他日志一样最新(“最新”的定义在下面)，那么它一定保存了所有已提交的日志。\nRequestVote RPC实现的限制: RPC 会包含关于 candidate 的日志信息，如果 voter 自己的日志比 candidate 的日志更新， 那么 vote 会拒绝投票。\n原文 Raft uses the voting process to prevent a candidate from winning an election unless its log contains all committed entries. A candidate must contact a majority of the cluster in order to be elected, which means that every committed entry must be present in at least one of those servers. If the candidate’s log is at least as up-to-date as any other log in that majority (where “up-to-date” is defined precisely below), then it will hold all the committed entries. The RequestVote RPC implements this restriction: the RPC includes information about the candidate’s log, and the voter denies its vote if its own log is more up-to-date than that of the candidate. Raft 通过比较最后一个日志的 index 以及 term来决定两个日志中的那个是最新的。 如果最后一条日志的 term 不同，则更大的那份就是更新的。 如果日志有相同的 term，那么哪个日志长(日志数组的长度？还是 index 的大小？)，哪个就是最新的。\n原文 Raft determines which of two logs is more up-to-date by comparing the index and term of the last entries in the logs. If the logs have last entries with different terms, then the log with the later term is more up-to-date. If the logs end with the same term, then whichever log is longer is more up-to-date. Committing entries from previous terms 如\u0026nbsp;Log replication 介绍的那样，只要日志被存储到大多数节点中，leader 就知道这条日志是可以 在当前 term 内被提交的。 如果 leader 在提交日志之前崩溃了，未来的 leader 将会尝试完成这条日志的复制。 然而，leader 不能立即推断出在前一个 term 的日志在保存到大多数服务器上时就一定被提交了。 图8说明了一条已经被存储到大多数节点上的老日志条目，也可能被未来的 leader 覆盖掉。\nFigure 8: 如图的时间序列展示了为什么领导人无法决定对老任期号的日志条目进行提交。在 (a) 中，S1 是领导人，部分的(跟随者)复制了索引位置 2 的日志条目。在 (b) 中，S1 崩溃了，然后 S5 在任期 3 里通过 S3、S4 和自己的选票赢得选举，然后从客户端接收了一条不一样的日志条目放在了索引 2 处。然后到 (c)，S5 又崩溃了；S1 重新启动，选举成功，开始复制日志。在这时，来自任期 2 的那条日志已经被复制到了集群中的大多数机器上，但是还没有被提交。如果 S1 在 (d) 中又崩溃了，S5 可以重新被选举成功（通过来自 S2，S3 和 S4 的选票），然后覆盖了他们在索引 2 处的日志。反之，如果在崩溃之前，S1 把自己主导的新任期里产生的日志条目复制到了大多数机器上，就如 (e) 中那样，那么在后面任期里面这些新的日志条目就会被提交（因为 S5 就不可能选举成功）。 这样在同一时刻就同时保证了，之前的所有老的日志条目就会被提交。 原文 As described in Section 5.3, a leader knows that an entry from its current term is committed once that entry is stored on a majority of the servers. If a leader crashes before committing an entry, future leaders will attempt to finish replicating the entry. However, a leader cannot immediately conclude that an entry from a previous term is committed once it is stored on a majority of servers. Figure 8 illustrates a situation where an old log entry is stored on a majority of servers, yet can still be overwritten by a future leader. 为了消除图8中的问题，Raft 永远不使用通过计算副本数量的方式去提交前一个 term 的日志。 只有 leader 当前 term 内的日志条目才会通过计算副本数量的方式来提交； 一旦当前 term 的日志以这种方式提交，那么由于Log Matching则之前的所有日志条目也被被提交。 在某些情况下，leader能安全的知道一个老的日志是否已经被提交(例如，该日志是否存储到服务器上)， 但是 Raft 为了简化问题使用了一种更加保守的方法。\n原文 To eliminate problems like the one in Figure 8, Raft never commits log entries from previous terms by counting replicas. Only log entries from the leader’s current term are committed by counting replicas; once an entry from the current term has been committed in this way, then all prior entries are committed indirectly because of the Log Matching Property. There are some situations where a leader could safely conclude that an older log entry is committed (for example, if that entry is stored on every server), but Raft takes a more conservative approach for simplicity 当 leader 复制之前任期里的日志时，Raft 会为所有日志保留原始的任期号，这在提交规则上产生了额外的复杂性。 在其他的共识算法中，如果一个新的 leader 要重新复制之前的任期里的日志时，它必须使用当前新的任期号。 Raft 使用的方法更加容易辨别出日志，因为它可以随着时间和日志的变化对日志维护着同一个任期编号。 另外，和其他的算法相比，Raft 中的new leader 只需要发送更少日志条目( 其他算法中必须在他们被提交之前发送更多的冗余日志条目来为他们重新编号)。\n原文 Raft incurs this extra complexity in the commitment rules because log entries retain their original term numbers when a leader replicates entries from previous terms. In other consensus algorithms, if a new leader rereplicates entries from prior “terms,” it must do so with its new “term number.” Raft’s approach makes it easier to reason about log entries, since they maintain the same term number over time and across logs. In addition, new leaders in Raft send fewer log entries from previous terms than in other algorithms (other algorithms must send redundant log entries to renumber them before they can be committed) Safety argument 在定义了完整的 Raft 算法后，我们现在可以更精确的说Leader Completeness完全成立。 我们假设Leader Completeness不成立，然后我们推出一个矛盾来。 假设任期T的 leader(leader T)在任期内提交了一条日志，但是这条日志没有存储到某个未来的 term 的 leader 的日志中。 设大于 T 最小任期 U，U 的 leader 没有这条日志。\nFigure 9: asd 原文 Given the complete Raft algorithm, we can now argue more precisely that the Leader Completeness Property holds (this argument is based on the safety proof; see Section 9.2). We assume that the Leader Completeness Property does not hold, then we prove a contradiction. Suppose the leader for term T (leaderT) commits a log entry from its term, but that log entry is not stored by the leader of some future term. Consider the smallest term U \u0026gt; T whose leader (leaderU) does not store the entry. 代码实现思路 2A 根据图2中的 state 这一节添加对应的属性 添加RaftRole属性，代表当前的角色: leader，candidate，follower 实现ticker这个函数: 判断是否很久没有收到心跳，来发起选举 判断是否需要发送心跳，来维持自己的权威 Links 项目地址: \u0026nbsp;https://pdos.csail.mit.edu/6.824/labs/lab-raft.html GFS 相关资料: \u0026nbsp;https://fzdwx.github.io/posts/2022-10-07-gfs/#links Raft paper: \u0026nbsp;https://pdos.csail.mit.edu/6.824/papers/raft-extended.pdf Raft paper 中文翻译: \u0026nbsp;https://github.com/maemual/raft-zh_cn/blob/master/raft-zh_cn.md Diagram of Raft interactions： \u0026nbsp;https://pdos.csail.mit.edu/6.824/notes/raft_diagram.pdf Students guid to Raft: \u0026nbsp;https://thesquareplanet.com/blog/students-guide-to-raft/ Raft locking: \u0026nbsp;https://pdos.csail.mit.edu/6.824/labs/raft-locking.txt Raft structure: \u0026nbsp;https://pdos.csail.mit.edu/6.824/labs/raft-structure.txt Paxos Replicated State Machines as the Basis of a High-Performance Data Store \u0026nbsp;https://static.usenix.org/event/nsdi11/tech/full_papers/Bolosky.pdf TiKV 对于 Raft 优化 \u0026nbsp;https://cn.pingcap.com/blog/optimizing-raft-in-tikv \u0026nbsp;https://www.cnblogs.com/niejunlei/p/9719557.html \u0026nbsp;https://blog.csdn.net/viskaz/article/details/124232474 \u0026nbsp;https://www.cnblogs.com/brianleelxt/p/13251540.html ","permalink":"https://fzdwx.github.io/posts/2022-10-10-raftkv/","summary":"Lab2文档翻译 由于我的英文不是很好，所以使用翻译软件进行翻译，然后人工进行校对进行理解。\n原文地址: \u0026nbsp;https://pdos.csail.mit.edu/6.824/labs/lab-raft.html Introduction 这是一系列实验中的第一个，我们将构建一个 fault-tolerant key/value storage system 。 在本实验中我们将实现 Raft (一种复制的状态机协议)。在下一个实验中，我们将在 Raft 上构建一个 key/value service 。 然后，您将在有多个副本的状态机上进行 shard(分片？根据 key 进行 hash 来决定路由到哪个副本上) 来提高性能。\n复制(replication)通过在多个复制服务器上存储其状态(即数据)的完整副本来实现 fault tolerance 。 即使有一些服务器出现 failure (崩溃或网络断开和抖动) replication 也允许它们继续运行。 挑战在于 failures 可能导致副本存在不同的数据。\nRaft 将客户端的请求组织成一个序列，被称为 log ，并且确保所有 replica servers 看到相同的 log 。 每个 replica 按照日志的顺序来执行客户端的请求，将它们应用于其本地的服务状态副本(就是运行来自客户端的命令)。 由于所有存活的副本读取的日志内容都是相同的，所以都以相同的顺序来执行请求，因此它们都有相同的服务状态。 如果一个服务器发生了 failure 但是后来又 recovery (恢复) 了，Raft 会负责将它的 log 更新到最新状态。只要至少大多数的服务器还活着，并且能够继续通信， 那么 Raft 将持续运行。如果没有到达这个数量，那么 Raft 将会停止运行，直到达到这个数量才会重新开始运行。\n在本 lab 中，你将把 Raft 实现为一个带有相关方法的 GO 的对象类型，目的是为了能在更大的模块中使用。 一组 Raft 实例通过 RPC 来维护 replicated logs。你的 Raft 实例将支持一连串不确定编号的 command， 也可以叫 log entries。 这些 entity 通过 index(索引)来进行编号。具有给定索引的 log entry 将被 commit， 此时，您的 Raft 应该将这条 log 发送到 larger service 上执行。","title":"Raft Kv"},{"content":"最近在学习 jyy 的\u0026nbsp; 计算机系统基础习题课 ， 在做\u0026nbsp; pa1 的时候提 到一个运行红白机游戏的\u0026nbsp; 项目 运行起来的效果 遇到的坑:\n1.SIGSTKSZ 参数找不到\nSIGSTKSZ参数找不到 将SIGSTKSZ修改为一个固定的参数\n//uint8_t sigstack[SIGSTKSZ]; uint8_t sigstack[8192]; 2.serial 不正确\nserial不正确 修改nemu/src/device/serial.c#init_serial，每个人的不同，我的是 9。\n","permalink":"https://fzdwx.github.io/posts/2022-10-09-mario-nes/","summary":"最近在学习 jyy 的\u0026nbsp; 计算机系统基础习题课 ， 在做\u0026nbsp; pa1 的时候提 到一个运行红白机游戏的\u0026nbsp; 项目 运行起来的效果 遇到的坑:\n1.SIGSTKSZ 参数找不到\nSIGSTKSZ参数找不到 将SIGSTKSZ修改为一个固定的参数\n//uint8_t sigstack[SIGSTKSZ]; uint8_t sigstack[8192]; 2.serial 不正确\nserial不正确 修改nemu/src/device/serial.c#init_serial，每个人的不同，我的是 9。","title":"在Abstract Machine上玩超级马里奥"},{"content":"宏定义与展开 宏展开: 通过 复制/粘贴 改变代码的形态\n一个include的例子 a.c:\n#include \u0026lt;stdio.h\u0026gt; int main(){ printf( #include \u0026lt;qwe\u0026gt; ); } qwe:\n\u0026#34;hello world\\n\u0026#34; 通过运行gcc a.c \u0026amp;\u0026amp; a.out得到\n❯ gcc a.c \u0026amp;\u0026amp; a.out hello world 一个define的例子 a.c:\n#define A \u0026#34;aaaaaaaaaaaa\u0026#34; #define TEN(A) A A A A A A A A A #define B TEN(A) #define C TEN(B) int main(int argc, char const *argv[]) { puts(C); return 0; } 可以通过gcc a.c -E来查看预编译的结\n❯ gcc a.c -E # 0 \u0026#34;a.c\u0026#34; # 0 \u0026#34;\u0026lt;built-in\u0026gt;\u0026#34; # 0 \u0026#34;\u0026lt;command-line\u0026gt;\u0026#34; # 1 \u0026#34;/usr/include/stdc-predef.h\u0026#34; 1 3 4 # 0 \u0026#34;\u0026lt;command-line\u0026gt;\u0026#34; 2 # 1 \u0026#34;a.c\u0026#34; int main(int argc, char const *argv[]) { puts(\u0026#34;aaaaaaaaaaaa\u0026#34; \u0026#34;aaaaaaaaaaaa\u0026#34; \u0026#34;aaaaaaaaaaaa\u0026#34; \u0026#34;aaaaaaaaaaaa\u0026#34; \u0026#34;aaaaaaaaaaaa\u0026#34; \u0026#34;aaaaaaaaaaaa\u0026#34; \u0026#34;aaaaaaaaaaaa\u0026#34; \u0026#34;aaaaaaaaaaaa\u0026#34; \u0026#34;aaaaaaaaaaaa\u0026#34; \u0026#34;aaaaaaaaaaaa\u0026#34; \u0026#34;aaaaaaaaaaaa\u0026#34; \u0026#34;aaaaaaaaaaaa\u0026#34; \u0026#34;aaaaaaaaaaaa\u0026#34; \u0026#34;aaaaaaaaaaaa\u0026#34; \u0026#34;aaaaaaaaaaaa\u0026#34; \u0026#34;aaaaaaaaaaaa\u0026#34; \u0026#34;aaaaaaaaaaaa\u0026#34; \u0026#34;aaaaaaaaaaaa\u0026#34; \u0026#34;aaaaaaaaaaaa\u0026#34; \u0026#34;aaaaaaaaaaaa\u0026#34; \u0026#34;aaaaaaaaaaaa\u0026#34; \u0026#34;aaaaaaaaaaaa\u0026#34; \u0026#34;aaaaaaaaaaaa\u0026#34; \u0026#34;aaaaaaaaaaaa\u0026#34; \u0026#34;aaaaaaaaaaaa\u0026#34; \u0026#34;aaaaaaaaaaaa\u0026#34; \u0026#34;aaaaaaaaaaaa\u0026#34; \u0026#34;aaaaaaaaaaaa\u0026#34; \u0026#34;aaaaaaaaaaaa\u0026#34; \u0026#34;aaaaaaaaaaaa\u0026#34; \u0026#34;aaaaaaaaaaaa\u0026#34; \u0026#34;aaaaaaaaaaaa\u0026#34; \u0026#34;aaaaaaaaaaaa\u0026#34; \u0026#34;aaaaaaaaaaaa\u0026#34; \u0026#34;aaaaaaaaaaaa\u0026#34; \u0026#34;aaaaaaaaaaaa\u0026#34; \u0026#34;aaaaaaaaaaaa\u0026#34; \u0026#34;aaaaaaaaaaaa\u0026#34; \u0026#34;aaaaaaaaaaaa\u0026#34; \u0026#34;aaaaaaaaaaaa\u0026#34; \u0026#34;aaaaaaaaaaaa\u0026#34; \u0026#34;aaaaaaaaaaaa\u0026#34; \u0026#34;aaaaaaaaaaaa\u0026#34; \u0026#34;aaaaaaaaaaaa\u0026#34; \u0026#34;aaaaaaaaaaaa\u0026#34; \u0026#34;aaaaaaaaaaaa\u0026#34; \u0026#34;aaaaaaaaaaaa\u0026#34; \u0026#34;aaaaaaaaaaaa\u0026#34; \u0026#34;aaaaaaaaaaaa\u0026#34; \u0026#34;aaaaaaaaaaaa\u0026#34; \u0026#34;aaaaaaaaaaaa\u0026#34; \u0026#34;aaaaaaaaaaaa\u0026#34; \u0026#34;aaaaaaaaaaaa\u0026#34; \u0026#34;aaaaaaaaaaaa\u0026#34; \u0026#34;aaaaaaaaaaaa\u0026#34; \u0026#34;aaaaaaaaaaaa\u0026#34; \u0026#34;aaaaaaaaaaaa\u0026#34; \u0026#34;aaaaaaaaaaaa\u0026#34; \u0026#34;aaaaaaaaaaaa\u0026#34; \u0026#34;aaaaaaaaaaaa\u0026#34; \u0026#34;aaaaaaaaaaaa\u0026#34; \u0026#34;aaaaaaaaaaaa\u0026#34; \u0026#34;aaaaaaaaaaaa\u0026#34; \u0026#34;aaaaaaaaaaaa\u0026#34; \u0026#34;aaaaaaaaaaaa\u0026#34; \u0026#34;aaaaaaaaaaaa\u0026#34; \u0026#34;aaaaaaaaaaaa\u0026#34; \u0026#34;aaaaaaaaaaaa\u0026#34; \u0026#34;aaaaaaaaaaaa\u0026#34; \u0026#34;aaaaaaaaaaaa\u0026#34; \u0026#34;aaaaaaaaaaaa\u0026#34; \u0026#34;aaaaaaaaaaaa\u0026#34; \u0026#34;aaaaaaaaaaaa\u0026#34; \u0026#34;aaaaaaaaaaaa\u0026#34; \u0026#34;aaaaaaaaaaaa\u0026#34; \u0026#34;aaaaaaaaaaaa\u0026#34; \u0026#34;aaaaaaaaaaaa\u0026#34; \u0026#34;aaaaaaaaaaaa\u0026#34; \u0026#34;aaaaaaaaaaaa\u0026#34; \u0026#34;aaaaaaaaaaaa\u0026#34; \u0026#34;aaaaaaaaaaaa\u0026#34;); return 0; } 修改true定义的例子 它只在特定的行数时才会为true。\na.c:\n#define true (__LINE__ % 2 != 0) #include \u0026lt;stdio.h\u0026gt; int main(int argc, char const *argv[]) { if(true){printf(\u0026#34;yes %d\\n\u0026#34;, __LINE__); } if(true){printf(\u0026#34;yes %d\\n\u0026#34;, __LINE__); } if(true){printf(\u0026#34;yes %d\\n\u0026#34;, __LINE__); } if(true){printf(\u0026#34;yes %d\\n\u0026#34;, __LINE__); } if(true){printf(\u0026#34;yes %d\\n\u0026#34;, __LINE__); } if(true){printf(\u0026#34;yes %d\\n\u0026#34;, __LINE__); } if(true){printf(\u0026#34;yes %d\\n\u0026#34;, __LINE__); } if(true){printf(\u0026#34;yes %d\\n\u0026#34;, __LINE__); } if(true){printf(\u0026#34;yes %d\\n\u0026#34;, __LINE__); } if(true){printf(\u0026#34;yes %d\\n\u0026#34;, __LINE__); } if(true){printf(\u0026#34;yes %d\\n\u0026#34;, __LINE__); } } 定义一个宏的参数为函数 #define NAMES(x) \\ x(Tom) x(Jerry) x(Tyke) x(Spike) int main(int argc, char const *argv[]) { #define PRINT(x) puts(\u0026#34;Hello,\u0026#34; #x \u0026#34;!\u0026#34;); NAMES(PRINT) } 可以看到，展开后是调用了4遍。\n❯ gcc -E a.c # 0 \u0026#34;a.c\u0026#34; # 0 \u0026#34;\u0026lt;built-in\u0026gt;\u0026#34; # 0 \u0026#34;\u0026lt;command-line\u0026gt;\u0026#34; # 1 \u0026#34;/usr/include/stdc-predef.h\u0026#34; 1 3 4 # 0 \u0026#34;\u0026lt;command-line\u0026gt;\u0026#34; 2 # 1 \u0026#34;a.c\u0026#34; int main(int argc, char const *argv[]) { puts(\u0026#34;Hello,\u0026#34; \u0026#34;Tom\u0026#34; \u0026#34;!\u0026#34;); puts(\u0026#34;Hello,\u0026#34; \u0026#34;Jerry\u0026#34; \u0026#34;!\u0026#34;); puts(\u0026#34;Hello,\u0026#34; \u0026#34;Tyke\u0026#34; \u0026#34;!\u0026#34;); puts(\u0026#34;Hello,\u0026#34; \u0026#34;Spike\u0026#34; \u0026#34;!\u0026#34;); } C程序执行的两个视角 静态: C代码连续的一段总能找到对应一段连续的机器指令 动态: C代码执行的状态总能对应到机器的状态\n源代码视角: 函数，变量，指针 机器指令视角: 寄存器，内存，地址 共同的地方: 内存\n代码，变量(源代码) = 地址 + 长度(机器指令) 内存 = 代码 + 数据 + 堆栈 所以理解C程序执行最重要的就时内存模型。\n","permalink":"https://fzdwx.github.io/posts/2022-10-09-c-compile/","summary":"宏定义与展开 宏展开: 通过 复制/粘贴 改变代码的形态\n一个include的例子 a.c:\n#include \u0026lt;stdio.h\u0026gt; int main(){ printf( #include \u0026lt;qwe\u0026gt; ); } qwe:\n\u0026#34;hello world\\n\u0026#34; 通过运行gcc a.c \u0026amp;\u0026amp; a.out得到\n❯ gcc a.c \u0026amp;\u0026amp; a.out hello world 一个define的例子 a.c:\n#define A \u0026#34;aaaaaaaaaaaa\u0026#34; #define TEN(A) A A A A A A A A A #define B TEN(A) #define C TEN(B) int main(int argc, char const *argv[]) { puts(C); return 0; } 可以通过gcc a.c -E来查看预编译的结\n❯ gcc a.c -E # 0 \u0026#34;a.","title":"C Compile"},{"content":" 为了性能(Performance), 所以将数据分割放到大量的服务器上，从而实现并行的读取数据，这就是分片(Sharding)。 而成败上千的机器总会发生错误，所以有了容错(Fault Tolerance)。 实现容错最简单的方式就是复制(Replication)，其中一个发生故障了就切换另一个。 使用了复制，如果你不够小心，那么它们之间就可能会不一致。数据就有可能出现问题，所以就有了不一致的问题(Inconsistency)。 如果为了实现一致性(Consistency)，那么就需要多进行额外的交互来保证一致性，所以代价就是低性能(Low Perf) ，但这与我们开始的希望不符合。 TIP So，强一致性代表着低性能。 设计目标 由于GFS是建立在大量的计算机上的，而这些计算机会不可避免的发生故障。所以必须要进行：检查，容错以及快速从故障恢复。 主要支持大文件(例如说好几个G的文件)，同时也支持小文件但不做针对性的优化。 工作负载主要由两种类型的读取组成：大的流式读取和小的随机读取 。对于性能有过特别考虑的应用通常会作批处理并且对他们读取的内容进行排序，这样可以使得他们的读取始终是单向顺序读取，而不需要往回读取数据。 在大的流式读取中，单个操作通常要读取数百k，甚至1m或者更大的数据。对于同一个客户端来说，往往会发起连续的读取操作顺序读取一个文件。 小的随机读取通常在某个任意的偏移位置读取几kb的数据。小规模的随机读取通常在文件的不同位置，读取几k数据。 GFS中的文件通常上一旦完成写入就很少会再次修改，所以主要针对大的流式读取，同时夜支持任意位置的小规模写入操作。 GFS对多个客户端并行添加同一个文件必须要有非常有效且明确语义的支持，即原子操作。通常会有多个客户端会并行的对同一个文件进行append。 高性能的稳定带宽的网络要比低延时更加重要。我们大多数的目标应用程序都非常重视高速批量处理数据 ，而很少有人对单个读写操作有严格的响应时间要求。 架构 单个master，多个chunk server(保存具体的文件)，多个client。 每个文件被拆分为一定大小(64mb)的块(chunk)，且每个chunk有一个唯一的64位的标志(chunk handle)。 每个chunk都会在不同的chunk server上保存备份(默认是3个)，用户可以指定不同的复制级别。 master管理元数据(metadata)，例如文件到chunk的映射关系，chunk的位置信息等。 master管理chunk的分片，孤点chunk的垃圾回收机制，chunk server之间的镜像管理等 每个chunk server与master之间有心跳机制，并在检测的过程中年发出指令并收集状态。 GFS Master中的metadata filename -\u0026gt; chunk ids(chunk handles) NV chunk handle与chunk数据的对应关系 chunk保存在哪个服务器上(chunk server list) chunk的version no NV chunk的primary chunk server，因为写操作在在其上进行 primary chunk server的lease expiration 这两个data table都在master的内存中存放，为了容错(例如说重启后数据不丢失数据)，它会在磁盘上存储log，读取的使用从内存里面读取，写的时候会写入内存以及磁盘。 每当有数据变更时，就会在磁盘上的日志进行追加，并且定期(日志增长超过某一个大小)创建checkpoint(类似快照，不用从头开始读取)\nGFS Read Steps 首先读请求就表明client有filename以及想要读取的位置(offset)，然后发送给master。 master收到请求后就从filenames中获取对应的chunk handles。而每个chunk的大小上固定的，所以就得到的具体开始的chunk handle。 然后根据chunk handle找到对应存放数据的chunk server的列表返回给client。 client可以选择一个server来进行读取(论文中说会选择一个最近的服务器，应为google里面ip是连续的，可以根据ip判断远近) ，应为客户端每次只读取1mb或者64kb的数据，所以它会缓存chunk与chunk server的关系，这样就不用每次都请求。 chunk server收到请求后，根据chunk handle(推测chunk是安装chunk handle进行命名的)找到对应的chunk以及offset对应的数据给客户端。 q1: 如果读取的数据跨越了一个chunk怎么办？ 例如说client想要读取的数据超过了64mb，或者仅仅上是2个byte却跨越了chunk，client会在发送请求时注意到这次请求跨越了边界， 所以会将一个请求拆分为2个请求发送到master，所以这里可能上向master发送两次读请求，之后在向不同的chunk server读取数据。\n多个副本之间变更顺序的一致性 针对一个chunk\nmaster授权给某个持有这个chunk的server一个租约期限(60s)，称为primary。 primary对所有的更改操作进行排序(serial order)，然后其他的secondary根据这个顺序进行变更。 只要这个chunk正在变更，那么primary就可以向master申请延长租约。 GFS Write Steps client向master发送请求获取chunk server list(primary,secondaries)， 如果没有primary，master就会选择一个secondary成为primary。 client获取到chunk server list后会缓存下来，只有当primary 没有响应或租约过期后才会再次请求。 client将数据推送到所有replicas，客户端不保证推送的顺序，每个chunk server会将数据保存在内部的lur cache中，直到数据被使用或过期。 当所有replicas都收到了数据，client将会发送一个写请求到primary，它标识了之前推送到每个副本的数据。 primary将这些写入组织成一定的顺序应用到自己本地。 primary然后将这个应用顺序转发给各个secondary。 secondaries应用这个顺序完成修改并答复primary。 primary答复client，如果出现了任意错误也会答复给client。在出现错误的情况下，write request也可能在primary以及secondary中成功 (如果primary直接就失败了，那么它将不会转发serial order给secondaries)，client将认为这次请求是失败的，它会通过重试来处理( 3-7尝试几次重新写入) GFS Atomic Record Appends 对同一片区域个并发写入是不可序列化的 这片区域可能最终包含多个客户端的数据片段。 一个原子的append操作。recored append至少会在给定的offset(GFS自己选择的，因为这里可能会失败，可能有一些chunk server上有这个数据) 上追加到文件上一次，并将该offset返回给client。它类似O_APPEND保证原子性。 recored append遵守\u0026nbsp; GFS Write Steps 流程，但是有一些特别的地方:\nclient推送所有数据后，primary会检查append到该chunk后是否超过了单个chunk的大小。 如果超过了，则在当前chunk填充到最大offset时(secondary也要保存)，回复client，指出该操作应该在下一个chunk上重试( record的大小需要控制在单个chunk最大值的四分之一，以保证碎片在可接收的水平)。 如果没有超过最大大小，则按照正常的情况进行保存。 过期副本检测 如果chunk server发生故障而宕机或者丢失了某些更新请求，那么它就有可能过期了。对于每个chunk，master都维护了一个version no来标识最新和过期的副本。\n当master为一个chunk的primary server授权或续期时就会增加version no并通知所有replicas进行更新。\n在数据一致的情况下，master和所有replicas的version no是一致的(在client发送写请求之前可以保证)。\n当chunk server重启或上报version no时，master会检查它时否包含过期的副本，如果发现master发现version no大于它的记录，master会采用更高的version no进行更新。\nmaster通过周期性的垃圾回收来删除过期的副本，在删除前，它会确认在它所有client的chunk信息请求的应答中没有包含这个过期的副本。\nclient在从master获取chunk server列表时会附带获取version no，所以它可以进行比对，选择最新的副本进行操作。\n总结 这并不是一个合格的多副本，多活，高可用，故障自修复的分布式系统。\nLinks \u0026nbsp;gfs paper 原文 \u0026nbsp;gfs paper 中文翻译 \u0026nbsp;gfs 视频 \u0026nbsp;gfs 视频翻译 \u0026nbsp;Bad Replication Design ","permalink":"https://fzdwx.github.io/posts/2022-10-07-gfs/","summary":"为了性能(Performance), 所以将数据分割放到大量的服务器上，从而实现并行的读取数据，这就是分片(Sharding)。 而成败上千的机器总会发生错误，所以有了容错(Fault Tolerance)。 实现容错最简单的方式就是复制(Replication)，其中一个发生故障了就切换另一个。 使用了复制，如果你不够小心，那么它们之间就可能会不一致。数据就有可能出现问题，所以就有了不一致的问题(Inconsistency)。 如果为了实现一致性(Consistency)，那么就需要多进行额外的交互来保证一致性，所以代价就是低性能(Low Perf) ，但这与我们开始的希望不符合。 TIP So，强一致性代表着低性能。 设计目标 由于GFS是建立在大量的计算机上的，而这些计算机会不可避免的发生故障。所以必须要进行：检查，容错以及快速从故障恢复。 主要支持大文件(例如说好几个G的文件)，同时也支持小文件但不做针对性的优化。 工作负载主要由两种类型的读取组成：大的流式读取和小的随机读取 。对于性能有过特别考虑的应用通常会作批处理并且对他们读取的内容进行排序，这样可以使得他们的读取始终是单向顺序读取，而不需要往回读取数据。 在大的流式读取中，单个操作通常要读取数百k，甚至1m或者更大的数据。对于同一个客户端来说，往往会发起连续的读取操作顺序读取一个文件。 小的随机读取通常在某个任意的偏移位置读取几kb的数据。小规模的随机读取通常在文件的不同位置，读取几k数据。 GFS中的文件通常上一旦完成写入就很少会再次修改，所以主要针对大的流式读取，同时夜支持任意位置的小规模写入操作。 GFS对多个客户端并行添加同一个文件必须要有非常有效且明确语义的支持，即原子操作。通常会有多个客户端会并行的对同一个文件进行append。 高性能的稳定带宽的网络要比低延时更加重要。我们大多数的目标应用程序都非常重视高速批量处理数据 ，而很少有人对单个读写操作有严格的响应时间要求。 架构 单个master，多个chunk server(保存具体的文件)，多个client。 每个文件被拆分为一定大小(64mb)的块(chunk)，且每个chunk有一个唯一的64位的标志(chunk handle)。 每个chunk都会在不同的chunk server上保存备份(默认是3个)，用户可以指定不同的复制级别。 master管理元数据(metadata)，例如文件到chunk的映射关系，chunk的位置信息等。 master管理chunk的分片，孤点chunk的垃圾回收机制，chunk server之间的镜像管理等 每个chunk server与master之间有心跳机制，并在检测的过程中年发出指令并收集状态。 GFS Master中的metadata filename -\u0026gt; chunk ids(chunk handles) NV chunk handle与chunk数据的对应关系 chunk保存在哪个服务器上(chunk server list) chunk的version no NV chunk的primary chunk server，因为写操作在在其上进行 primary chunk server的lease expiration 这两个data table都在master的内存中存放，为了容错(例如说重启后数据不丢失数据)，它会在磁盘上存储log，读取的使用从内存里面读取，写的时候会写入内存以及磁盘。 每当有数据变更时，就会在磁盘上的日志进行追加，并且定期(日志增长超过某一个大小)创建checkpoint(类似快照，不用从头开始读取)\nGFS Read Steps 首先读请求就表明client有filename以及想要读取的位置(offset)，然后发送给master。 master收到请求后就从filenames中获取对应的chunk handles。而每个chunk的大小上固定的，所以就得到的具体开始的chunk handle。 然后根据chunk handle找到对应存放数据的chunk server的列表返回给client。 client可以选择一个server来进行读取(论文中说会选择一个最近的服务器，应为google里面ip是连续的，可以根据ip判断远近) ，应为客户端每次只读取1mb或者64kb的数据，所以它会缓存chunk与chunk server的关系，这样就不用每次都请求。 chunk server收到请求后，根据chunk handle(推测chunk是安装chunk handle进行命名的)找到对应的chunk以及offset对应的数据给客户端。 q1: 如果读取的数据跨越了一个chunk怎么办？ 例如说client想要读取的数据超过了64mb，或者仅仅上是2个byte却跨越了chunk，client会在发送请求时注意到这次请求跨越了边界， 所以会将一个请求拆分为2个请求发送到master，所以这里可能上向master发送两次读请求，之后在向不同的chunk server读取数据。","title":"GFS"},{"content":"查看当前backlight由什么控制:\n一般都是intel。\nls /sys/class/backlight 查看当前的亮度:\ncat /sys/class/backlight/intel_backlight/max_brightness 修改亮度:\necho 5000 | sudo tee /sys/class/backlight/intel_backlight/brightness ","permalink":"https://fzdwx.github.io/posts/2022-10-04-backlight/","summary":"由于使用DWM，它不能像KDE那样之间有图形化的亮度调节功能，所以记录一下。","title":"调节linux屏幕的亮度"},{"content":" ranger 配置推荐 \u0026nbsp;https://zhuanlan.zhihu.com/p/105731111 \u0026nbsp;https://zhuanlan.zhihu.com/p/441083543 fzf \u0026nbsp;https://github.com/junegunn/fzf ranger \u0026nbsp;https://github.com/ranger/ranger 压测工具 \u0026nbsp;https://github.com/link1st/go-stress-testing ","permalink":"https://fzdwx.github.io/notes/1/","summary":" ranger 配置推荐 \u0026nbsp;https://zhuanlan.zhihu.com/p/105731111 \u0026nbsp;https://zhuanlan.zhihu.com/p/441083543 fzf \u0026nbsp;https://github.com/junegunn/fzf ranger \u0026nbsp;https://github.com/ranger/ranger 压测工具 \u0026nbsp;https://github.com/link1st/go-stress-testing ","title":"终端下好用的程序"},{"content":"安装: git clone https://git.suckless.org/dwm cd dwm sudo make clean install 启动 如果已经有了sddm，可以使用sudo systemctl disable sddm.service 来进行关闭，如果到时候不想用了可以用sudo systemctl enable sddm.service来开启sddm。\n# 准备配置文件 cp /etc/X11/xinit/xinitrc ~/.xinitrc vim ~/.xinitrc # 添加 exec dwm # 注释 #twm \u0026amp; #xclock -geometry 50x50-1+1 \u0026amp; #xterm -geometry 80x50+494+51 \u0026amp; #xterm -geometry 80x20+494-0 \u0026amp; #exec xterm -geometry 80x66+0+0 -name login 然后重启并调用startx即可。\n遇到的问题 直接断网 使用NetworkManager进行连接，可能要用到dhcpcd。\nnmcli device wifi list nmcli device wifi connect {{wifi name}} password {{pwd}} 可能会连接失败，你可以试试nmcli connection show 来查看是否存在以前的连接信息，可以用nmcli connection delete {{wifi name}}来删除对应的信息\n输入法失效 添加配置文件\nvim ~/.pam_environment INPUT_METHOD DEFAULT=fcitx5 GTK_IM_MODULE DEFAULT=fcitx5 QT_IM_MODULE DEFAULT=fcitx5 XMODIFIERS DEFAULT=@im=fcitx5 添加启动命令\nvim ~/.xinitrc1 # 在exec dwm之前添加 exec fcitx5 \u0026amp; 扩展屏幕无效 # 可以先查看有哪些屏幕 xrandr # # 将下面的命令加入 ~/.xinitrc1 中，在exec dwm之前添加 # 我有两个，是上下关系，所以用below 其他有 left 以及 right xrandr --output eDP-1-1 --auto --below HDMI-0 常用快捷键 可以参考我配置： \u0026nbsp;https://github.com/fzdwx/dwm MOD -\u0026gt; WIN\n快捷键 desc MOD+SHIFT+ENTER 新开终端 在本tag内切换聚焦窗口 快捷键 desc ALT + tab 切换聚焦窗口 MOD + up 切换聚焦窗口 MOD + down 切换聚焦窗口 跨tag操作 快捷键 desc MOD + left / right 切换tag MOD + SHIGT + left / tight 将当前窗口移动到其他tag MOD + a 第一次是显示所有tag，第二次是跳转到聚焦的窗口所在的窗口 窗口操作 快捷键 desc MOD + q 退出窗口 MOD + h 隐藏窗口 MOD + SHIFT + h 显示窗口（像一个栈一样。） MOD + ENTER 将当前窗口设置为主窗口 MOD + f 将当前窗口全屏 MOD + CTRL + 方向键 移动窗口 MOD + ALT + 方向键 调整窗口大小 MOD + o 只显示当前窗口/显示所有窗口 MOD + t 开启/关闭 聚焦目标的浮动模式 MOD + SHIFT + t 开启/关闭 全部目标的浮动模式 | MOD + SHIFT + f | 开启/关闭 状态栏 |\n切换屏幕 快捷键 desc MOD + x 将鼠标移动到其他屏幕 MOD + SHIFT + X 将当前窗口移动到其他屏幕 Links \u0026nbsp;ArchWiki Dwm \u0026nbsp;Dwm patches \u0026nbsp;Dwm patches中文翻译 \u0026nbsp;picom美化 ","permalink":"https://fzdwx.github.io/posts/2022-09-29-dwm/","summary":"安装: git clone https://git.suckless.org/dwm cd dwm sudo make clean install 启动 如果已经有了sddm，可以使用sudo systemctl disable sddm.service 来进行关闭，如果到时候不想用了可以用sudo systemctl enable sddm.service来开启sddm。\n# 准备配置文件 cp /etc/X11/xinit/xinitrc ~/.xinitrc vim ~/.xinitrc # 添加 exec dwm # 注释 #twm \u0026amp; #xclock -geometry 50x50-1+1 \u0026amp; #xterm -geometry 80x50+494+51 \u0026amp; #xterm -geometry 80x20+494-0 \u0026amp; #exec xterm -geometry 80x66+0+0 -name login 然后重启并调用startx即可。\n遇到的问题 直接断网 使用NetworkManager进行连接，可能要用到dhcpcd。\nnmcli device wifi list nmcli device wifi connect {{wifi name}} password {{pwd}} 可能会连接失败，你可以试试nmcli connection show 来查看是否存在以前的连接信息，可以用nmcli connection delete {{wifi name}}来删除对应的信息","title":"Dwm初体验"},{"content":" 由于最近切换到了linux，不可避免的经常需要使用vi等编辑器，所以这次好好折腾一下。\n我的配置地址: \u0026nbsp;https://github.com/fzdwx/nvim 。\n我的配置 pakcer 我选用的插件管理器是packer，我也不知道选什么好，就按最新的来吧。\n--- setup packer local fn = vim.fn local install_path = fn.stdpath(\u0026#39;data\u0026#39;) .. \u0026#39;/site/pack/packer/start/packer.nvim\u0026#39; if fn.empty(fn.glob(install_path)) \u0026gt; 0 then packer_bootstrap = fn.system({ \u0026#39;git\u0026#39;, \u0026#39;clone\u0026#39;, \u0026#39;--depth\u0026#39;, \u0026#39;1\u0026#39;, \u0026#39;https://github.com/wbthomason/packer.nvim\u0026#39;, install_path }) vim.cmd(\u0026#34;packadd packer.nvim\u0026#34;) end --- add plugins require(\u0026#39;packer\u0026#39;).startup(function(use) -- 自托管 use \u0026#39;wbthomason/packer.nvim\u0026#39; -- 其他插件 -- 如果是第一次则同步 if packer_bootstrap then require(\u0026#39;packer\u0026#39;).sync() end end) \u0026nbsp;alpha 是一个dashboard，也可以说是一个欢迎界面。\nrequire(\u0026#39;packer\u0026#39;).startup(function(use) -- ... --- 添加下面两行进行安装 use \u0026#34;goolord/alpha-nvim\u0026#34;; use \u0026#34;kyazdani42/nvim-web-devicons\u0026#34;; -- ... end) 配置代码太长了就不放了，可以点击这里\u0026nbsp; 这里 参考。\n\u0026nbsp;telescope 主要作用是文件的查找与预览。\nrequire(\u0026#39;packer\u0026#39;).startup(function(use) -- ... --- 添加下面代码进行安装 use { \u0026#39;nvim-telescope/telescope.nvim\u0026#39;, tag = \u0026#39;0.1.0\u0026#39;, \u0026#34;ahmedkhalf/project.nvim\u0026#34;, \u0026#39;nvim-lua/plenary.nvim\u0026#39;, } -- ... end) \u0026nbsp;配置 \u0026nbsp;key map 一些教程 \u0026nbsp;Rust and nvim ","permalink":"https://fzdwx.github.io/posts/2022-09-28-neovim-use-notes/","summary":"由于最近切换到了linux，不可避免的经常需要使用vi等编辑器，所以这次好好折腾一下。","title":"Neovim使用记录  "},{"content":" HTTP 1.1之前的实现就不讨论了，因为它们已经过时太久了，我上网的时候就已经接触不到了，所以主要说说HTTP/1.1、HTTP/2。\nHTTP/1.1 HTTP/1.1协议报文简介 CRLF: \\r\\n\nMETHOD: HTTP请求，GET、POST、PUT、DELETE\u0026hellip;\nURI: 统一资源标识符，例如/，/index.html\u0026hellip;\nHTTPVersion: HTTP协议的版本号，例如HTTP/1.1，HTTP/2\nHEADERS: 请求头，例如Host:localhost，Accept: */*。\nBODY: 请求体，例如说一个JSON数据{\u0026quot;name\u0026quot;:\u0026quot;fzdwx\u0026quot;}\nHTTPStatus: HTTP响应状态，常见的有200，404等\nHTTPStatusDesc: HTTP响应状态描述，200对应的OK。\n请求 METHOD\u0026lt;SPACE\u0026gt;URI\u0026lt;SPACE\u0026gt;HTTPVersion HEADERS \u0026lt;CRLF\u0026gt; BODY 示例:\nGET /hello HTTP/1.1 Host: 192.168.1.107:8889 Connection: keep-alive Cache-Control: max-age=0 Upgrade-Insecure-Requests: 1 User-Agent: Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/99.0.4844.82 Safari/537.36 Accept: text/html,application/xhtml+xml,application/xml;q=0.9,image/avif,image/webp,image/apng,*/*;q=0.8,application/signed-exchange;v=b3;q=0.9 Accept-Encoding: gzip, deflate Accept-Language: zh-CN,zh;q=0.9 响应 HTTPVersion HTTPStatus HTTPStatusDesc HEADERS \u0026lt;CRLF\u0026gt; BODY 示例:\n如果响应中使用了transfer-encoding: chunked这个来替代Content-Length ，就表示这是一个不固定大小的响应，结尾通常用0\\r\\n来分割。\nHTTP/1.1 200 OK transfer-encoding: chunked content-type: application/json; charset=utf-8 0/r/n HTTP/1.1主要新特性 默认是长连接(Connection: Keep-alive)，支持一个TCP连接处理多个请求。 缓存策略，在请求头中使用Cache-Control，Expires，Last-Modified，ETag等来控制。 允许响应分块，就是上面提到的transfer-encoding: chunked，允许服务端可以多次返回响应体。 但是还是存在一定的问题，例如说如果有一个TCP连接阻塞了，还是会开启新的TCP连接进行处理请求。\nH2 HTTP2中的主要概念:\nConnection: 一个TCP连接包含一个或多个Stream,所有的通讯都在一个TCP连接上完成。 Stream: 一个可以双向通讯的数据流，包含一条或多条Message，每个数据流都一个唯一标识符以及可选的优先级信息。 Message: 对应HTTP/1.1中的请求或响应，包含一条或多条Frame。 Frame: 最小传输单位，它以二进制进行编码。 \u0026nbsp;HTTP通讯简图 在HTTP/1.1中是有Start Line + header + body 组成的，而在H2中是由一个HEADER Frame以及多个DATA Frame组成的。\nHTTP/1.1与H2报文组成的区别 Frame 通常有一些公共的字段，例如Length，Type，Flags以及Stream Id；也各个类型所独有的字段。\n分类如下:\n\u0026nbsp;DATA : 用于传输http消息体。 \u0026nbsp;HEADERS : 用于传输首部字段。 \u0026nbsp;PRIORITY : 用于指定或重新指定引用资源的优先级。 \u0026nbsp;RST_STREAM : 用于通知流的非正常终止。 \u0026nbsp;SETTINGS : 用于约定客户端和服务端的配置数据。例如设置初识的双向流量控制窗口大小。 \u0026nbsp;PUSH_PROMISE : 服务端推送许可。 \u0026nbsp;PING : 用于计算往返时间，执行“ 活性” 检活。 \u0026nbsp;GOAWAY : 用于通知对端停止在当前连接中创建流。 \u0026nbsp;WINDOW_UPDATE : 用于调整个别流或个别连接的流量。 \u0026nbsp;CONTINUATION : 专门用于传递较大 HTTP 头部时的持续帧。 为什么H2必须要走HTTPS？ 这其实在H2标准中没有规定，主要是为了更方便的进行HTTP协议的 升级/协商，确认一个Web服务器是否支持H2通常有两种方式:\n在请求头中设置Upgrade: HTTP/2.0以及Connection: Upgrade,HTTP2-Settings等，类似升级到Websocket。 使用TLS中的ALPN(Application Layer Protocol Negotiation，应用层协议协商)中的ALPN Next Protocol 字段，在Client Hello与Server Hello这个阶段就可以确定下来。 而现在的浏览器基本都是实现的方式二，即与HTTPS绑定在一起。但是如果我们不用浏览器进行访问，当然也可以不用HTTPS。\n详细可\u0026nbsp;参考 。\n为什么H2能实现并行响应请求? 在HTTP/1.1中，请求与响应是一一对应的，在同一个连接里，客户端依次发送两个请求，一段时间以后收到来自服务器的一个响应，这个响应一定是对应于第一个发出去的请求的。 因为没有一个标志来表示哪个响应对应哪个请求。\n而在H2中基于Stream和Frame的设计: 每个Frame都带有Stream Id来标识是否为同一个Stream里面的数据，每个Stream 互不影响，这样就能做到在一个TCP里面连接里面传输多对请求/响应。\nH2的新特性 H2的对HTTP/1.1优化的核心就是 使用尽可能少的连接数。\n多路复用: 只用一个TCP连接就能处理多对 请求/响应 ，不用在开启另外的TCP连接，就是通过Stream与Frame来实现的。 二进制分帧: 使用Frame为最小单位进行通讯，并采用二进制编码。 \u0026nbsp;头部压缩 : 使用HPACK算法进行优化. 维护一份相同的\u0026nbsp;静态字典 ，包含常见的请求头的KV组合 一份动态字典，可以动态的扩容(每个连接单独维护) 支持哈夫曼编码(\u0026nbsp;静态哈夫曼码表 ) 在HTTP/1中消息体可以用gzip进行压缩，但是请求头通常没有任何压缩，有时候请求头的数据可能比请求体的数据还多。\n请求优先级: 一般在HEADERS帧与PRIORITY帧中携带，通常依赖于服务端的支持程度。 工具 生成测试签名 go run $GOROOT/src/crypto/tls/generate_cert.go --host localhost 使用curl调试HTTPS curl https://zcygov.cn -vv Links \u0026nbsp;Hypertext Transfer Protocol Version 2 (HTTP/2) \u0026nbsp;HPACK: Header Compression for HTTP/2 \u0026nbsp;HTTP/2资料汇总 \u0026nbsp;HTTP/2中帧的定义 \u0026nbsp;HTTP/2新的机遇与挑战 \u0026nbsp;探索http1.0到http3.0的发展史，详解http2.0 \u0026nbsp;HTTP/2相比1.0有哪些重大改进 ","permalink":"https://fzdwx.github.io/posts/2022-09-28-http-protocol/","summary":"简介HTTP/1.1与H2。","title":"HTTP协议"},{"content":"看的时候很懵，写的时候更懵，我以为是要写几个MapReduce的程序，在把官方的A few rules以及Hints( 翻译可以看\u0026nbsp;这里 ) 仔细的阅读了几遍后才发现是要写一个 MapReduce 的调度框架。。。\n解题思路可以分为 3 步。\nworker与master先通过rpc进行通信，即worker发送请求，master能够正确的应答且worker 能够收到。这一部分其实示例代码已经给到了，可以模仿着写一个从master获取需要执行的任务名称的rpc request。 上面一步完成后就可以进行真正的Map task请求以及执行了: Map task何时生成？ 每个worker只向master请求一次就好了吗？ Map task执行 超时/失败 需要有重试机制吗？ 如何保证Map task消费时的线程安全？ 当所有的Map task执行完毕后就需要执行Reduce task了，如果你写完了第 2 步，那么Reduce task其实就是水到渠成了。 生成Reduce task，这里的每个Reduce task需要加载的文件按什么分成一组？ 剩下的与Map task处理方法相同。 我就是卡在了Reduce task创建时文件分组这个地方了，我直接把同一个Map task生成的文件当成一组了\u0026hellip;但是显然不是这样。\n","permalink":"https://fzdwx.github.io/posts/2022-09-27-mit6.824-lab1/","summary":"about mit6.824 lab1(Map reduce framework)","title":"MapReduce 框架实现思路"},{"content":" 今天发现了一个 linux 下的 application launcher \u0026nbsp;rofi ，它可以快速切换窗口和启动程序， 我用它和wmctrl进行配合使用。\n我的使用过程:\n1.Archlinux install\nyay -S rofi 2.添加自定义快捷键绑定 hotkey 为alt+space触发rofi -show。\n3.pressalt+space，然后使用shift+leftorright进行切换 mode 。\n更改主题以及显示 icon:\n1.生成配置文件\nmkdir -p ~/.config/rofi rofi -dump-config \u0026gt; ~/.config/rofi/config.rasi 2.显示icon\nsed -i \u0026#39;8c show-icons: true;\u0026#39; ~/.config/rofi/config.rasi 3.更换主题\n参考 \u0026nbsp;https://github.com/lr-tech/rofi-themes-collection#installing-themes 我的wmctrl的配置示例:\n使用xprop WM_CLASS获取前缀。\nalias chrome=\u0026#34;wmctrl -x -a google-chrome || google-chrome-stable \u0026gt; /dev/null 2\u0026gt;\u0026amp;1 \u0026amp;\u0026#34; alias note=\u0026#34;wmctrl -x -a obsidian || /usr/bin/obsidian \u0026gt; /dev/null 2\u0026gt;\u0026amp;1 \u0026amp;\u0026#34; alias codew=\u0026#34;wmctrl -x -a code || /opt/code/code \u0026gt; /dev/null 2\u0026gt;\u0026amp;1 \u0026amp;\u0026#34; alias idea=\u0026#34;wmctrl -x -a jetbrains-idea || /opt/idea/bin/idea.sh \u0026gt; /dev/null 2\u0026gt;\u0026amp;1 \u0026amp;\u0026#34; alias discord=\u0026#34;wmctrl -x -a discord || /opt/discord/Discord \u0026gt; /dev/null 2\u0026gt;\u0026amp;1 \u0026amp;\u0026#34; ","permalink":"https://fzdwx.github.io/posts/2022-09-18-about-rofi/","summary":"一个 linux 下的 application launcher, 让你只用键盘就可以进行应用的启动与切换。","title":"About rofi"},{"content":"","permalink":"https://fzdwx.github.io/flomo/","summary":"","title":""}]