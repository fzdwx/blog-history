---
title: "Raft Kv"
date: 2022-10-10T22:26:55+08:00
draft: false
tags: ["mit6.824"]
---

## Lab2文档翻译

### Introduction

这是一系列实验中的第一个，我们将构建一个*fault-tolerant key/value storage system*。
在本实验中我们将实现*Raft(一种复制的状态机协议)*。在下一个实验中，我们将在Raft上构建一个key/value service。
然后，您将在多个复制的状态机上进行shard来提高性能。

复制的服务通过在多个复制服务器上存储其状态(即数据)的完整副本来实现*fault tolerance*。
即使有一些服务器出现故障(崩溃或网络断开和抖动)replication也允许它们继续运行。
挑战在于**failures可能导致副本存在不同的数据**。

Raft将客户端的请求组织成一个序列，被成为log，并且确保所有replica servers看到相同的log。
每个副本按照日志的顺序来执行客户端的请求，将它们应用于其本地的服务状态副本。
由于**所有存活的副本读取的日志内容都是相同的，所以都以相同的顺序来执行请求，因此它们都有相同的服务状态**。
如果一个服务器失败了但是后来又恢复来，Raft会复制把它的日志更新。只要至少大多数的服务器还或者，并且能够继续通信，
那么Raft将继续运行。如果没有到达这个数量，那么Raft将会停止运行，直到到达这个数量才会重新开始。

在本lab中，你将把Raft实现为一个带有相关方法的GO的对象类型，目的是为了能在更大的模块中使用。
一组Raft实例通过RPC来维护replicated logs。你的Raft实例将支持一连串不确定编号(数量?)的command，
也可以叫log entries。 这些entries通过索引来进行编号。具有给定索引的log entry将被提交，此时，
您的Raft应该将这个条log发送到更大的服务上执行。

你应该遵循 [extended Raft paper](https://pdos.csail.mit.edu/6.824/papers/raft-extended.pdf)中设计，
特别是图 2.你将实现论文宏的大 部分内容，包括**保存持久化状态**和节**点故障自动重启后读取状态**。
你将不会实现集群成员的变化(Section 6)。

你可能会发现这个 [指南](https://thesquareplanet.com/blog/students-guide-to-raft/)很有用，
还有这个关于concurrency的[锁](https://pdos.csail.mit.edu/6.824/labs/raft-locking.txt)
和[结构](https://pdos.csail.mit.edu/6.824/labs/raft-structure.txt)的建议，
如果需要更广泛的视角，可以看看Paxos, Chubby, Paxos Made Live, Spanner, Zookeeper, Harp, Viewstamped Replication和
[Bolosky et al](https://static.usenix.org/event/nsdi11/tech/full_papers/Bolosky.pdf)。

请记住，本 lab 中最具挑战性的部分可能不是实现你的解决方案，而是调试它。为了帮助应对这一挑战，你可能需要把事件花在如何使你的实现更容易调试。
你可以参考 [指导页](https://pdos.csail.mit.edu/6.824/labs/guidance.html)和这篇关于有效打印声明的
[博文](https://blog.josejg.com/debugging-pretty/)。

我们还提供了 [Raft 交互图](https://pdos.csail.mit.edu/6.824/notes/raft_diagram.pdf)，
可以帮助阐明Raft代码如何与上层(使用者?)交互。

### The code

通过向`raft/raft.go`添加代码来实现Raft。在该文件中，你会发现骨架代码，以及如何发送和接收 RPC 的例子。
你的实现必须支持以下接口，测试者和（最终）你的键/值服务器将使用该接口。你可以在`raft.go`的注释中找到更多细节。

{{< block type="tip">}}
raft 实例只能通过 rpc 进行通信且必须使用`labrpc`这个包(例如不能使用文件以及共享变量)。
{{< /block >}}

```go
// create a new Raft server instance:
rf := Make(peers, me, persister, applyCh)

// start agreement on a new log entry:
rf.Start(command interface{}) (index, term, isleader)

// ask a Raft for its current term, and whether it thinks it is leader
rf.GetState() (term, isLeader)

// each time a new entry is committed to the log, each Raft peer
// should send an ApplyMsg to the service (or tester).
type ApplyMsg
```

#### Make(peers []*labrpc.ClientEnd, me int,persister *Persister, applyCh chan ApplyMsg)

用于创建 raft server。

1. 所有的 raft server 的端口都在`peers[]`存放(包括当前的服务)，当前服务的端口可以通过`peers[me]`来获取。
2. 所有的服务的`perrs[]`数组都具有相同的顺序。
3. `presister`是一个用来存放`persistent state`的地方，并且在初始的时候会保存最具的状态，如果有。
4. `applyCh`是 service 或 tester 发送消息给 raft 的通道。`Make()`
   必须快速返回，所以它应该为一些长时间运行的任务启动`goruntines`。

#### Start(command interface{}) (int, int, bool)

使用Raft的服务(e.g a k/v server)希望就下一个要追加到raft日志的命令达成一致(就是追加到 raft
日志的下一条命令是相同的？)。如果当前raft server不是leader则返回false。
否则启动协议并**立即返回**，无需等待日志追加完成。
**所以无法保证次命令将一定会被提交到raft日志中，因为leader可能会失败或者在选举中失败**。
即使raft实例被kill这个函数也应该return gracefully。

第一个返回值是该命令出现的索引，如果它曾经被提交的话。第二个返回值是当前的术语(???)。如果这个服务器认为它是领导者，第三个返回值是真。

每个新提交的`raft log entity`都应该发送一个`AppliMsg`到`Make()`的`applyCh`中。

### 2A

实现Raft leader election以及heartbeats(`AppendEntries`RPCs没有log entries。空的的意思?)。

2A的目标是: 选出一个leader，如果没有失败，它仍然是leader，如果old leader失败或者与old leader之间的数据包发生丢失则由new
leader接管。

{{< block type="tip">}}
这个失败是 leader 出现故障的意思？就是说只要它没出现运行故障或者网络问题就永远是leader？
{{< /block >}}

要点:

1. 通过运行`go test -run 2A`来进行测试你的实现。
2. 按照论文的图 2，主要关系发送和接收`RequestVote RPCs`，与`the Rules for Servers that relate to elections`
   以及`the State related to leader election`。
3. 添加图 2 中与 leader election 相关的状态到`Raft`这个结构体中，且还需要定义一个结构来保存每个日志的信息。
4. 实现`RequestVote()`，这样 raft 服务们就能互相投票了。添加`RequestVOteArgs`和`RequestVoteReply`者两个结构体。修改`Make()`
   ，创建一个 goroutine，用于检查心跳消息，如果有一段时间没有收到 peer 的消息时将发送`RequestVote`RPCs 来定期发起领导者选举。这样，如果有
   leader 了，peer 将知道谁是 leader，或者自己成为 leader。
5. 实现心跳，需要定义一个`AppendEntries`RPC 结构(尽管你可能还不需要所有参数)，
   并且让leader定期发送它。编写一个`AppendEntries`RPC 的 handle method，用于重置选举超时，
   这样当有一个人已经当选时，其他服务器不会又成为leader。
6. 确保不同peer的选举超时不在同一时间发生，否则所有peer将只为自己投票，这样就没有人会成为leader了。
7. 在测试时，leader每秒发送的RPC请求不能超过 10 次。
8. 在测试时，要求raft在old leader失败后5秒内选举new leader(如果大多数节点仍然能继续通讯)。但是请记住，如果出现`split vote(
   如果数据包丢失或者候选人选择了相同的随机退避时间就有可能发生)`，leader选举可能需要多轮。所以必须设置足够短的选举超时(
   也就是心跳间隔)，即使会选举多轮，也有可能在5秒内完成。
9. 论文的第5.2节提到的选举超时范围是150到300毫秒。只有当leader发送心跳的频率大大高于150毫秒一次时，上面论文提到的范围才有意义。
   由于在测试时限制每秒10次心跳，所以必须使用比论文中更大的选举超时时间，但是不能太大，因为可能会无法在5秒内完成选举。
10. 如果您的代码无法通过测试，请再次阅读论文中的图2，leader选举的全部逻辑分布在图中多个部分。
11. 不要忘记实现`GetState()`。
12. 在测试时，如果要关闭一个raft实例，会调用`rf.kill()`。我们可以调用`rf.killed`来检查是否被调用了`kill()`。您可能希望在所有的循环中都这样
    做，以避免死亡的Raft实例打印混乱的信息。
13. `GO RPC`只发送名称以大写字母开头的结构体字段。子结构体也必须拥有大写的字段名。

## Raft论文翻译

> 选取一些重要的片段进行翻译

### Introduction

raft算法和已经存在的共识算法在某些地方很相似(主要是Oki以及Liskov's的Viewstamped Replication)，但是它有以下新特性:

> raft is similar in many ways to existing consensus al-gorithms (most notably, Oki and Liskov’s Viewstamped
> Replication), but it has several novel features:

- 强领导者: Raft使用一种比其他共识算法更强的领导形式。例如，日志只从leader发送给其他服务器。这简化了对复制日志的管理，使的Raft更容易理解。
- 领导选举: Raft使用随机定时器来选取leader。这种方式仅仅是在所有共识算法都需要改进的心跳机制上有些许改进，然而这使得Raft在解决冲突时更简单和快速。
- 成员调整: 集群中更改server时，Raft使用了新的联合共识(join consensus)算法，
  两种不同的配置的majorities在变更期间重叠(overlap)， 允许集群在配置变更的时候，持续正常运行。

> - Strong leader: Raft uses a stronger form of leadership than other consensus algorithms. For example,log entries
    only flow from the leader to other servers. This simplifies the management of the replicated log and makes Raft
    easier to understand.
> - Leader election: Raft uses randomized timers to elect leaders. This adds only a small amount of mechanism to the
    heartbeats already required for any consensus algorithm, while resolving conflicts simply and rapidly.
> - Membership changes: Raft’s mechanism for changing the set of servers in the cluster uses a new joint consensus
    approach where the majorities of two different configurations overlap during transitions. This allows the cluster
    to continue operating normally during configuration changes.

### Replicated State Machine

`复制状态机(Replicated State Machine)`在分布式系统中被用于解决各种容错问题。例如GFS,HDFS,RAMCloud等单leader的大型集群系统，通常使用独立
的复制状态机来管理领导选举和存储配置信息来保证在leader崩溃会存活下来，复制状态机的例子包括Chubby以及Zookeeper。

![Figure 1: 复制状态机架构。共识算法管理来自客户端的包含状态机命令的复制日志，状态机按照相同的顺序来处理它们，所以它们产生相同的输出。](/images/7.png)

共识算法通常出现在复制状态机的上下文中，在这种方法中，在一组server上的状态机对同一个的状态会计算出相同的副本，即使一些server宕机也可以继续运行。

> Replicated state machines are used to solve a variety of fault tolerance problems in distributed systems. For example,
> large-scale systems that have a single cluster leader, such as GFS, HDFS, and RAMCloud, typically use a separate
> replicated state machine to manage leader election and store configuration information that must survive leader
> crashes. Examples of replicated state machines include Chubby and ZooKeeper. Consensus algorithms typically arise in
> the context of replicated state machines.In this approach, state machines on a collection of servers compute identical
> copies of the same state and can continue operating even if some of the servers are down.

复制状态机通过**复制日志实现**，如图一所示。每个服务保存包含一系列命令的日志，其状态机按照顺序来执行它们。
每个**日志包含相同顺序的相同命令**，所以每个状态机处理相同的命令序列。因为**状态机是确定的**，
所以每个状态机**会计算出相同的状态**和**相同顺序的输出**。

> Replicated state machines are typically implemented using a replicated log, as shown in Figure 1. Each server stores a
> log containing a series of commands, which its state machine executes in order. Each log contains the same commands in
> the same order, so each state machine processes the same sequence of commands. Since the state machines are
> deterministic, each computes the same state and the same sequence of outputs.

共识算法的任务是**保证复制日志的一致性**。服务器上的共识模块接收来自客户端的命令并把它们添加到日志中，
并与其他服务器上的共识模块进行通讯以确保它们的每一条日志最终都相同(相同的请求有相同的顺序)， 即使有一些服务失败了。一旦命令被正确的复制，
每一个服务的状态机会按照日志的顺序去处理它们，然后将结果返回给客户端。

因此，这些服务似乎成为了一个单一的，高度可靠的状态机。

> Keeping the replicated log consistent is the job of the consensus algorithm. The consensus module on a server receives
> commands from clients and adds them to its log. It communicates with the consensus modules on other servers to ensure
> that every log eventually contains the same requests in the same order, even if some servers fail. Once commands are
> properly replicated, each server’s state machine processes them in log order, and the outputs are returned to clients.
> As a result, the servers appear to form a single, highly reliable state machine.

在实际的共识算法通常有以下属性:

- 确保非拜占庭(non-Byzantine)条件下的*安全性*(永远不返回错误的结果)，包括网络延迟，分区以及网络数据包丢失、冗余、乱序。
- 只要大多数的服务都在运行并能相互通信且和客户端通信，它们就能发挥出全部的功能(_可用性_)。因此，一个5台服务的集群能容忍2台服务出现故障。
  假定服务应为停机而出现故障，它们可能稍后会从stable storage`中恢复状态并从新加入集群。
- 不依赖与timing来保证日志的一致性: 错误的时钟和极端的信息延迟延迟在最坏的情况下会导致可用性问题。
- 在一般情况下，一个命令的完成在于集群中的大多数对单轮远程调用作出响应，少数低水平的服务不会影响系统的整体性能。

> Consensus algorithms for practical systems typically have the following properties:
> - They ensure _safety_ (never returning an incorrect result) under all non-Byzantine conditions,
    including network delays, partitions, and packet loss, duplication, and reordering.
> - They are fully functional (_available_) as long as any
    majority of the servers are operational and can communicate with each other and with clients. Thus, a
    typical cluster of five servers can tolerate the failure of any two servers. Servers are assumed to fail by
    stopping; they may later recover from state on stable storage and rejoin the cluster.
> - They do not depend on timing to ensure the consistency of the logs: faulty clocks and extreme message
    delays can, at worst, cause availability problems.
> - In the common case, a command can complete as soon as a majority of the cluster has responded to a single round of
    remote procedure calls; a minority of low servers need not impact overall system performance.

### The Raft consensus algorithm

Raft就是用于管理上一解描述的复制日志的算法。[图2](/posts/2022-10-10-raftkv/#figure-2)
是对该算法的精简型式的总结，[图3](/posts/2022-10-10-raftkv/#figure-3)列出来该算法的关键属性，接下来对这些部分进行逐一讨论。

> Raft is an algorithm for managing a replicated log of the form described in Section 2. Figure 2 summarizes the
> algorithm in condensed form for reference, and Figure 3 lists key properties of the algorithm; the elements of these
> figures are discussed piecewise over the rest of this section.

Raft首先通过选举出一位 *leader* 来实现共识，然后由leader完全管理日志复制。 // todo

> Raft implements consensus by first electing a distinguished leader, then giving the leader complete responsibility for
> managing the replicated log. The leader accepts log entries from clients, replicates them on other servers, and tells
> servers when it is safe to apply log entries to their state machines. Having a leader simplifies the management of the
> replicated log. For example, the leader can decide where to place new entries in the log without consulting other
> servers, and data flows in a simple fashion from the leader to other servers. A leader can fail or become disconnected
> from the other servers, in which case a new leader is elected.

#### Figure 2

![Figure 2: Raft共识算法的精简摘要(不包括成员更改以及日志压缩)。左上角的服务器行为被描述为一组独立且重复触发的规则。](/images/raftp2.png)

**state**:

- 在所有服务器上持久化(在响应RPCs之前进行更新)
    - `currentTerm`: 服务器知道的最后的任期号(初始0，单调递增)
    - `votedFor`: 当前服务器投给哪个服务器？
    - `log[]`: 日志，包含要执行的命令以及收到该日志的时间。
- 在所有服务器上不稳定存在
    - `commitIndex`: 已知的提交的日志中的最大索引(初始0，单调递增)
    - `lastApplied`: 状态机执行的日志中的最大索引(初始0，单调递增)
- 在leader上不稳定存在(在每次重新选举后初始化)
    - `nextIndex[]`: 对于每一个服务器，记录需要发给它的下一个日志条目的索引(初始为leader的最后一条日志索引+1)
    - `matchIndex[]`: 对于每一个服务器，记录已经复制到该服务器的日志的最高索引值(初始0，单调递增)

**AppendEntries RPC**

> 由leader发起调用来复制日志，同时也用于心跳检测

- Arguments:
    - `term`: leader的任期
    - `leaderId`: 用于follower找到leader
    - `prevLogIndex`: 前一个日志的索引
    - `prevLogTerm`: 前一个日志的`term`
    - `entries[]`: 用于存放日志(为空时是心跳检测，可能一次会发送多条来提升效率)
    - `leaderCommit`: leader的`commitIndex`////
- Results:
    - `term`: `currentTerm`，用于leader更新自己的`term`
    - `success`: 如果follower的`pervLogIndex`以及`prevLogTerm`能够匹配上则为true
- Receiver implementation:
    - `if term < currentTerm then return false`(如果 term < currentTerm返回 false)
    - `if log[prevLogIndex].term != prevLogTerm then return false`(如果在prevLogIndex处的日志的任期号与prevLogTerm不匹配时，返回
      false)
    - `if log[oldIndex].term != log[newIndex].term then remove log[oldIndex,lastIndex]`(
      如果一个已经存在的日志与新的日志冲突(_`index`相同但是`term`不同_)，则删除该索引处以及之后的所有日志)
    - 添加在日志列表中不存在的新日志
    - `if leaderCommit > commitIndex then commitIndex = min(leaderCommit,log[].last.commitIndex)`(如果leaderCommit >
      commitIndex，将commitIndex设置为leaderCommit和最新日志条目索引号中较小的一个)

**RequestVote RPC**

> 候选人调用，收集选票

- Arguments:
    - `term`: candidate的任期号
    - `candidateId`: 发起请求的candidate的id
    - `lastLogIndex`: candidate的最后一条日志的索引
    - `lastLogTerm`: candidate最后一条日志对应的任期号
- Results:
    - term: `currentTerm`，用于candidate更新自己的`term`
    - voteGranted: true表示候选人获得了选票
- Receiver implementation:
    - `if term < currentTerm then return false`(如果term < currentTerm返回 false)
    - `if (votedFor is null or votedFor == candidateId) and (lastLogIndex,lastLogTerm) == log[].last then return true`
      (如果votedFor为空或者与candidateId相同，并且候选人的日志和自己的日志一样新，则给该候选人投票)

**Rules for Servers**

- All Servers:
    - `if commitIndex > lastApplied then incr lastApplied and exec log[lastApplied]`（如果commitIndex >
      lastApplied，lastApplied自增，将log[lastApplied]应用到状态机）
    - `if appendEntries.logs exist (log.term > currentTerm) then currentTerm = log.term and set status = follower`(如果
      RPC 的请求或者响应中包含一个 term T 大于 currentTerm，则currentTerm赋值为 T，并切换状态为追随者follower)
- Followers:
    - 不会发出任何请求，只会对来自candidates以及leader的请求做出响应
    - 选举超时后，如果未收到当前leader的`AppendEntries RPC`或没有收到其他candidates的投票请求:则转换为candidate
- Candidates:
    - 转换成candidate之后开始选举
        - incr `currentTerm`
        - 投票给自己
        - reset election timer
        - 发送`RequestVote RPC`给其他所有服务器
    - 如果收到了多数的选票则成为leader
    - 如果收到new leader的`AppendEntries RPC`则成为follower
    - 如果选举超时则开始新一轮的选举
- Leaders:
    - 选举时: 向其他服务器发送空的`AppendEntries RPC`，在空闲时重复发送以防止选举超时
    - 如果收到来自客户端的命令: 添加到本地日志，在执行并作用到状态机后作出响应
    - 对于follower`if last log index  >= nextIndex`(如果上一次收到的日志的索引大于这次要发送给它的日志的索引(
      nextIndex)):
      则通过`AppendEntries RPC`将nextIndex之后的所有日志都发送发送出去
        - 如果成功: 将该follower的`nextIndex`以及`matchIndex`更新
        - 如果因为日志不一致导致失败: `nextIndex`递减并重新发送
    - 如果存在一个数N，满足`N > commitIndex`，大多数的`matchIndex[i] >= N`
      以及`log[N].term == currentTerm`: `set commitIndex = N`

#### Figure 3

![Figure 3: Raft保证这些属性在在任何时候都上正确的。](/images/f3.png)

- **Election Safety:** 在给定term内只能选出一个leader
- **Leader Append-Only**: leader永远不覆盖或删除日志，只会添加
- **Log Matching**: 如果两个日志在包含相同的index以及term，那么就认定它们完全相同
- **Leader Completeness**: 如果一条日志在给定的term内提交，那么它一定会出现在term更大的leader的日志中
- **State Machine Safety:** 如果一个服务器已经将给定索引位置的日志条目应用到状态机之中，则其他所有服务器不会在相同索引处出现不同的日志

## Links

1. 项目地址: https://pdos.csail.mit.edu/6.824/labs/lab-raft.html
2. GFS 相关资料: https://fzdwx.github.io/posts/2022-10-07-gfs/#links
3. Raft paper: https://pdos.csail.mit.edu/6.824/papers/raft-extended.pdf
4. Diagram of Raft interactions： https://pdos.csail.mit.edu/6.824/notes/raft_diagram.pdf
5. Students guid to Raft: https://thesquareplanet.com/blog/students-guide-to-raft/
6. Raft locking: https://pdos.csail.mit.edu/6.824/labs/raft-locking.txt
7. Raft structure: https://pdos.csail.mit.edu/6.824/labs/raft-structure.txt
8. Paxos Replicated State Machines as the Basis of a High-Performance Data
   Store https://static.usenix.org/event/nsdi11/tech/full_papers/Bolosky.pdf
9. https://www.cnblogs.com/niejunlei/p/9719557.html
10. https://blog.csdn.net/viskaz/article/details/124232474
11. https://www.cnblogs.com/brianleelxt/p/13251540.html