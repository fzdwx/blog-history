---
title: "Raft Kv"
date: 2022-10-10T22:26:55+08:00
draft: false
tags: ["mit6.824"]
---

## Introduction

这是一系列实验中的第一个，我们将构建一个`fault-tolerant key/value storage system`。在本实验中我们将实现`Raft`(一
种复制的状态机协议)。在下一个实验中，我们将在`Raft`上构建一个`key/value service`。然后，您将在多个复制的状态机上进行`shard`来提高性能。

复制的服务通过在多个复制服务器上存储其状态(即数据)的完整副本来实现`fault tolerance`。即使有一些服务器出现故障(崩溃或网络断开和抖动)`replication`也允许它们继续运行。挑战在于**failures 可能导致副本存在不同的数据**。

`Raft`将客户端的请求组织成一个序列，被成为`log`，并且确保所有`replica servers`看到相同的`log`。每个副本按照日志的顺序来执行客户端的请求，将它们应用于其本地的服务状态副本。由于**所有存活的副本读取的日志内容都是相同的，所以都以相同的顺序来执行请求，因此它们都有相同的服务状态**。如果一个服务器失败了但是后来又恢复来，`Raft`会复制把它的日志更新。只要至少大多数的服务器还或者，并且能够继续通信，那么`Raft`将继续运行。如果没有到达这个数量，那么`Raft`将会停止运行，直到到达这个数量才会重新开始。

在本 lab 中，你将把`Raft`实现为一个带有相关方法的 GO 的对象类型，目的是为了能在更大的模块中使用。一组`Raft`实例通过`RPC`来维护`replicated logs`。你的`Raft`实例将支持一连串不确定编号(数量?)的`command`，也可以叫`log entries`。这些`entries`
通过索引来进行编号。具有给定索引的`log entry`将被提交，此时，您的`Raft`应该将这个条`log`发送到更大的服务上执行。

你应该遵循 [extended Raft paper](https://pdos.csail.mit.edu/6.824/papers/raft-extended.pdf)中设计，特别是图 2.你将实现论文宏的大部分内容，包括**保存持久化状态**和节**点故障自动重启后读取状态**。你将不会实现集群成员的变化(Section 6)。

你可能会发现这个 [指南](https://thesquareplanet.com/blog/students-guide-to-raft/)很有用，还有这个关于 concurrency 的 [锁](https://pdos.csail.mit.edu/6.824/labs/raft-locking.txt)和 [结构](https://pdos.csail.mit.edu/6.824/labs/raft-structure.txt)的建议，如果需要更广泛的视角，可以看看`Paxos, Chubby, Paxos Made Live, Spanner, Zookeeper, Harp, Viewstamped Replication` 和 [Bolosky et al](https://static.usenix.org/event/nsdi11/tech/full_papers/Bolosky.pdf)。

请记住，本 lab 中最具挑战性的部分可能不是实现你的解决方案，而是调试它。为了帮助应对这一挑战，你可能需要把事件花在如何使你的实现更容易调试。你可以参考 [指导页](https://pdos.csail.mit.edu/6.824/labs/guidance.html)和这篇关于有效打印声明的 [博文](https://blog.josejg.com/debugging-pretty/)。

我们还提供了 [Raft 交互图](https://pdos.csail.mit.edu/6.824/notes/raft_diagram.pdf)，可以帮助阐明 `Raft` 代码如何与上层(使用者?)交互。

## The code

通过向`raft/raft.go`添加代码来实现`Raft`。在该文件中，你会发现骨架代码，以及如何发送和接收 RPC 的例子。你的实现必须支持以下接口，测试者和（最终）你的键/值服务器将使用该接口。你可以在`raft.go`的注释中找到更多细节。

{{< block type="tip">}}
raft 实例只能通过 rpc 进行通信且必须使用`labrpc`这个包(例如不能使用文件以及共享变量)。
{{< /block >}}

```go
// create a new Raft server instance:
rf := Make(peers, me, persister, applyCh)

// start agreement on a new log entry:
rf.Start(command interface{}) (index, term, isleader)

// ask a Raft for its current term, and whether it thinks it is leader
rf.GetState() (term, isLeader)

// each time a new entry is committed to the log, each Raft peer
// should send an ApplyMsg to the service (or tester).
type ApplyMsg
```

### Make(peers []*labrpc.ClientEnd, me int,persister *Persister, applyCh chan ApplyMsg)

用于创建 raft server。

1. 所有的 raft server 的端口都在`peers[]`存放(包括当前的服务)，当前服务的端口可以通过`peers[me]`来获取。
2. 所有的服务的`perrs[]`数组都具有相同的顺序。
3. `presister`是一个用来存放`persistent state`的地方，并且在初始的时候会保存最具的状态，如果有。
4. `applyCh`是 service 或 tester 发送消息给 raft 的通道。`Make()`必须快速返回，所以它应该为一些长时间运行的任务启动`goruntines`。

### Start(command interface{}) (int, int, bool)

使用 rafr 的服务(e.g a k/v server)希望就下一个要追加到 raft 日志的命令达成一致(就是追加到 raft 日志的下一条命令是相同的？)。如果当前 raft server 不是 leader，则返回 false。否则启动协议并**立即返回**，无需等待日志追加完成。**所以无法保证次命令将一定会被提交到 raft 日志中，因为 leader 可能会失败或者在选举中失败**。即使 raft 实例被 kill，这个函数也应该`retrun gracefully`。

第一个返回值是该命令出现的索引，如果它曾经被提交的话。第二个返回值是当前的术语(???)。如果这个服务器认为它是领导者，第三个返回值是真。

每个新提交的`raft log entity`都应该发送一个`AppliMsg`到`Make()`的`applyCh`中。

## 2A

实现 Raft leader election 以及 heartbeats(`AppendEntries` RPCs 没有`log entries`.空的的意思?)。

2A 的目标是: 选出一个 leader，如果没有失败，它仍然是 leader，如果 old leader 失败或者与 old leader 之间的数据包发生丢失则由 new leader 接管。

{{< block type="tip">}}
这个失败是 leader 出现故障的意思？就是说只要它没出现运行故障或者网络问题就永远是 leader？
{{< /block >}}

要点:

1. 通过运行`go test -run 2A`来进行测试你的实现。
2. 按照论文的图 2，主要关系发送和接收`RequestVote RPCs`，与`the Rules for Servers that relate to elections`以及`the State related to leader election`。
3. 添加图 2 中与 leader election 相关的状态到`Raft`这个结构体中，且还需要定义一个结构来保存每个日志的信息。
4. 实现`RequestVote()`，这样 raft 服务们就能互相投票了。添加`RequestVOteArgs`和`RequestVoteReply`者两个结构体。修改`Make()`，创建一个 goroutine，用于检查心跳消息，如果有一段时间没有收到 peer 的消息时将发送`RequestVote`RPCs 来定期发起领导者选举。这样，如果有 leader 了，peer 将知道谁是 leader，或者自己成为 leader。
5. 实现心跳，需要定义一个`AppendEntries`RPC 结构(尽管你可能还不需要所有参数)，并且让 leader 定期发送它。编写一个`AppendEntries`RPC 的 handle method，用于重置选举超时，这样当有一个人已经当选时，其他服务器不会又成为 leader。
6. 确保不同 peer 的选举超时不在同一时间发生，否则所有 peer 将只为自己投票，这样就没有人会成为 leader 了。
7. 在测试是时，leader 每秒发送的 RPC 请求不能超过 10 次。
8. // he tester requires your Raft to elect a new leader within five 

## Links

1. 项目地址: https://pdos.csail.mit.edu/6.824/labs/lab-raft.html
2. GFS 相关资料: https://fzdwx.github.io/posts/2022-10-07-gfs/#links
3. Raft paper: https://pdos.csail.mit.edu/6.824/papers/raft-extended.pdf
4. Diagram of Raft interactions： https://pdos.csail.mit.edu/6.824/notes/raft_diagram.pdf
5. Students guid to Raft: https://thesquareplanet.com/blog/students-guide-to-raft/
6. Raft locking: https://pdos.csail.mit.edu/6.824/labs/raft-locking.txt
7. Raft structure: https://pdos.csail.mit.edu/6.824/labs/raft-structure.txt
8. Paxos Replicated State Machines as the Basis of a High-Performance Data Store https://static.usenix.org/event/nsdi11/tech/full_papers/Bolosky.pdf
