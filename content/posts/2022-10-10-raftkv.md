---
title: "Raft Kv"
date: 2022-10-10T22:26:55+08:00
draft: false
tags: ["mit6.824"]
---

## Lab2文档翻译

### Introduction

这是一系列实验中的第一个，我们将构建一个`fault-tolerant key/value storage system`。在本实验中我们将实现`Raft`(一
种复制的状态机协议)。在下一个实验中，我们将在`Raft`上构建一个`key/value service`。然后，您将在多个复制的状态机上进行`shard`来提高性能。

复制的服务通过在多个复制服务器上存储其状态(即数据)的完整副本来实现`fault tolerance`。即使有一些服务器出现故障(崩溃或网络断开和抖动)`replication`也允许它们继续运行。挑战在于**failures 可能导致副本存在不同的数据**。

`Raft`将客户端的请求组织成一个序列，被成为`log`，并且确保所有`replica servers`看到相同的`log`。每个副本按照日志的顺序来执行客户端的请求，将它们应用于其本地的服务状态副本。由于**所有存活的副本读取的日志内容都是相同的，所以都以相同的顺序来执行请求，因此它们都有相同的服务状态**。如果一个服务器失败了但是后来又恢复来，`Raft`会复制把它的日志更新。只要至少大多数的服务器还或者，并且能够继续通信，那么`Raft`将继续运行。如果没有到达这个数量，那么`Raft`将会停止运行，直到到达这个数量才会重新开始。

在本 lab 中，你将把`Raft`实现为一个带有相关方法的 GO 的对象类型，目的是为了能在更大的模块中使用。一组`Raft`实例通过`RPC`来维护`replicated logs`。你的`Raft`实例将支持一连串不确定编号(数量?)的`command`，也可以叫`log entries`。这些`entries`
通过索引来进行编号。具有给定索引的`log entry`将被提交，此时，您的`Raft`应该将这个条`log`发送到更大的服务上执行。

你应该遵循 [extended Raft paper](https://pdos.csail.mit.edu/6.824/papers/raft-extended.pdf)中设计，特别是图 2.你将实现论文宏的大部分内容，包括**保存持久化状态**和节**点故障自动重启后读取状态**。你将不会实现集群成员的变化(Section 6)。

你可能会发现这个 [指南](https://thesquareplanet.com/blog/students-guide-to-raft/)很有用，还有这个关于 concurrency 的 [锁](https://pdos.csail.mit.edu/6.824/labs/raft-locking.txt)和 [结构](https://pdos.csail.mit.edu/6.824/labs/raft-structure.txt)的建议，如果需要更广泛的视角，可以看看`Paxos, Chubby, Paxos Made Live, Spanner, Zookeeper, Harp, Viewstamped Replication` 和 [Bolosky et al](https://static.usenix.org/event/nsdi11/tech/full_papers/Bolosky.pdf)。

请记住，本 lab 中最具挑战性的部分可能不是实现你的解决方案，而是调试它。为了帮助应对这一挑战，你可能需要把事件花在如何使你的实现更容易调试。你可以参考 [指导页](https://pdos.csail.mit.edu/6.824/labs/guidance.html)和这篇关于有效打印声明的 [博文](https://blog.josejg.com/debugging-pretty/)。

我们还提供了 [Raft 交互图](https://pdos.csail.mit.edu/6.824/notes/raft_diagram.pdf)，可以帮助阐明 `Raft` 代码如何与上层(使用者?)交互。

### The code

通过向`raft/raft.go`添加代码来实现`Raft`。在该文件中，你会发现骨架代码，以及如何发送和接收 RPC 的例子。你的实现必须支持以下接口，测试者和（最终）你的键/值服务器将使用该接口。你可以在`raft.go`的注释中找到更多细节。

{{< block type="tip">}}
raft 实例只能通过 rpc 进行通信且必须使用`labrpc`这个包(例如不能使用文件以及共享变量)。
{{< /block >}}

```go
// create a new Raft server instance:
rf := Make(peers, me, persister, applyCh)

// start agreement on a new log entry:
rf.Start(command interface{}) (index, term, isleader)

// ask a Raft for its current term, and whether it thinks it is leader
rf.GetState() (term, isLeader)

// each time a new entry is committed to the log, each Raft peer
// should send an ApplyMsg to the service (or tester).
type ApplyMsg
```

#### Make(peers []*labrpc.ClientEnd, me int,persister *Persister, applyCh chan ApplyMsg)

用于创建 raft server。

1. 所有的 raft server 的端口都在`peers[]`存放(包括当前的服务)，当前服务的端口可以通过`peers[me]`来获取。
2. 所有的服务的`perrs[]`数组都具有相同的顺序。
3. `presister`是一个用来存放`persistent state`的地方，并且在初始的时候会保存最具的状态，如果有。
4. `applyCh`是 service 或 tester 发送消息给 raft 的通道。`Make()`必须快速返回，所以它应该为一些长时间运行的任务启动`goruntines`。

#### Start(command interface{}) (int, int, bool)

使用 rafr 的服务(e.g a k/v server)希望就下一个要追加到 raft 日志的命令达成一致(就是追加到 raft 日志的下一条命令是相同的？)。如果当前 raft server 不是 leader，则返回 false。否则启动协议并**立即返回**，无需等待日志追加完成。**所以无法保证次命令将一定会被提交到 raft 日志中，因为 leader 可能会失败或者在选举中失败**。即使 raft 实例被 kill，这个函数也应该`retrun gracefully`。

第一个返回值是该命令出现的索引，如果它曾经被提交的话。第二个返回值是当前的术语(???)。如果这个服务器认为它是领导者，第三个返回值是真。

每个新提交的`raft log entity`都应该发送一个`AppliMsg`到`Make()`的`applyCh`中。

### 2A

实现 Raft leader election 以及 heartbeats(`AppendEntries` RPCs 没有`log entries`.空的的意思?)。

2A 的目标是: 选出一个 leader，如果没有失败，它仍然是 leader，如果 old leader 失败或者与 old leader 之间的数据包发生丢失则由 new leader 接管。

{{< block type="tip">}}
这个失败是 leader 出现故障的意思？就是说只要它没出现运行故障或者网络问题就永远是 leader？
{{< /block >}}

要点:

1. 通过运行`go test -run 2A`来进行测试你的实现。
2. 按照论文的图 2，主要关系发送和接收`RequestVote RPCs`，与`the Rules for Servers that relate to elections`以及`the State related to leader election`。
3. 添加图 2 中与 leader election 相关的状态到`Raft`这个结构体中，且还需要定义一个结构来保存每个日志的信息。
4. 实现`RequestVote()`，这样 raft 服务们就能互相投票了。添加`RequestVOteArgs`和`RequestVoteReply`者两个结构体。修改`Make()`，创建一个 goroutine，用于检查心跳消息，如果有一段时间没有收到 peer 的消息时将发送`RequestVote`RPCs 来定期发起领导者选举。这样，如果有 leader 了，peer 将知道谁是 leader，或者自己成为 leader。
5. 实现心跳，需要定义一个`AppendEntries`RPC 结构(尽管你可能还不需要所有参数)，并且让 leader 定期发送它。编写一个`AppendEntries`RPC 的 handle method，用于重置选举超时，这样当有一个人已经当选时，其他服务器不会又成为 leader。
6. 确保不同 peer 的选举超时不在同一时间发生，否则所有 peer 将只为自己投票，这样就没有人会成为 leader 了。
7. 在测试时，leader 每秒发送的 RPC 请求不能超过 10 次。
8. 在测试时，要求raft在old leader失败后5秒内选举new leader(如果大多数节点仍然能继续通讯)。但是请记住，如果出现split vote(如果数据包丢失或者候选人选择了相同的随机退避时间就有可能发生)，leader选举可能需要多轮。所以必须设置足够短的选举超时(也就是心跳间隔)，即使会选举多轮，也有可能在5秒内完成。
9. 论文的第5.2节提到的选举超时范围是150到300毫秒。只有当leader发送心跳的频率大大高于150毫秒一次时，上面论文提到的范围才有意义。由于在测试时限制每秒10次心跳，所以必须使用比论文中更大的选举超时时间，但是不能太大，因为可能会无法在5秒内完成选举。
10. 如果您的代码无法通过测试，请再次阅读论文中的图2，leader选举的全部逻辑分布在图中多个部分。
11. 不要忘记实现`GetState()`。
12. 在测试时，如果要关闭一个raft实例，会调用`rf.kill()`。我们可以调用`rf.killed`来检查是否被调用了`kill()`。您可能希望在所有的循环中都这样做，以避免死亡的Raft实例打印混乱的信息。
13. `GO RPC`只发送名称以大写字母开头的结构体字段。子结构体也必须拥有大写的字段名。

## Raft论文翻译

> 选取一些重要的片段进行翻译

### Introduction

raft is similar in many ways to existing consensus al-gorithms (most notably, Oki and Liskov’s Viewstamped Replication [29, 22]), but it has several novel features:

> raft算法和已经存在的一致性算法在某些地方很相似(主要是Oki以及Liskov's的Viewstamped Replication)，但是它有以下新特性:

- **Strong leader**: Raft uses a stronger form of leadership than other consensus algorithms. For example,log entries only flow from the leader to other servers. This simplifies the management of the replicated log and makes Raft easier to understand.
- **Leader election**: Raft uses randomized timers to elect leaders. This adds only a small amount of mechanism to the heartbeats already required for any consensus algorithm, while resolving conflicts simply and rapidly.
- **Membership changes**: Raft’s mechanism for changing the set of servers in the cluster uses a new joint consensus approach where the majorities of two different configurations overlap during transitions. This allows the cluster to continue operating normally during configuration changes.

> - 强领导者: Raft使用一种比其他一致性算法更强的领导形式。比如，日志只从leader发送给其他服务器。这简化了对复制日志的管理，似的Raft更容易理解。
> - 领导选举: Raft使用随机定时器来选取leader。这种方式仅仅是在所有一致性算法都需要改进的心跳机制上有些许改进，然而这使得Raft在解决冲突时更简单和快速。
> - 成员调整: 在实现调整集群中成员的机制时，Raft使用了新的联合一致性(join consensus)算法。在这种方法中，大多数两种不同配置的机器在转换关系时会交叠(overlap)。这使得在配置改变时，集群能够继续工作。


## Links

1. 项目地址: https://pdos.csail.mit.edu/6.824/labs/lab-raft.html
2. GFS 相关资料: https://fzdwx.github.io/posts/2022-10-07-gfs/#links
3. Raft paper: https://pdos.csail.mit.edu/6.824/papers/raft-extended.pdf
4. Diagram of Raft interactions： https://pdos.csail.mit.edu/6.824/notes/raft_diagram.pdf
5. Students guid to Raft: https://thesquareplanet.com/blog/students-guide-to-raft/
6. Raft locking: https://pdos.csail.mit.edu/6.824/labs/raft-locking.txt
7. Raft structure: https://pdos.csail.mit.edu/6.824/labs/raft-structure.txt
8. Paxos Replicated State Machines as the Basis of a High-Performance Data Store https://static.usenix.org/event/nsdi11/tech/full_papers/Bolosky.pdf
